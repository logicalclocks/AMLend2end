{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we will ingest computed node embeddings as feature groups and also prepare training datasets for anomaly detection model training   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Model Repository for best node embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>36</td><td>application_1607949680860_0038</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0038/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworksdavit-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e02_1607949680860_0038_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"NodeEmbeddings\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'application_1607949680860_0037_1'"
     ]
    }
   ],
   "source": [
    "best_model['experimentId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and load wights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "\n",
    "import pandas as pd\n",
    "from stellargraph import StellarDiGraph\n",
    "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "from stellargraph.data import UnsupervisedSampler, BiasedRandomWalk\n",
    "from stellargraph.layer import Node2Vec\n",
    "import pydoop.hdfs as pydoop\n",
    "import hsfs\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining StellarDiGraph"
     ]
    }
   ],
   "source": [
    "node_data = pd.DataFrame(node_pdf[['tx_behavior_id','prior_sar','initial_deposit']], index=node_pdf['id'])\n",
    "\n",
    "print('Defining StellarDiGraph')\n",
    "G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json\n",
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_number = 2\n",
    "walk_length = 2\n",
    "batch_size = 1\n",
    "emb_size = args_dict['emb_size'][0]\n",
    "# Extracting node embeddings\n",
    "walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "generator = Node2VecLinkGenerator(G, batch_size)\n",
    "\n",
    "node2vec = Node2Vec(emb_size, generator=generator)\n",
    "x_inp, x_out = node2vec.in_out_tensors()\n",
    "\n",
    "x_inp_src = x_inp[0]\n",
    "x_out_src = x_out[0]\n",
    "embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x7fe10af37410>"
     ]
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(\"hdfs:///Projects/{}/Experiments/\".format(hdfs.project_name()) + best_model['experimentId'])\n",
    "embedding_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "node_gen = Node2VecNodeGenerator(G, batch_size).flow(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "pdf = pd.DataFrame(embedding_model.predict(node_gen), index=G.nodes())\n",
    "emb_feature_names = [\"em_\" + str(c)  for c in pdf.columns]\n",
    "pdf.columns = emb_feature_names\n",
    "pdf['id'] = pdf.index\n",
    "node_embeddings_df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+-------------------+--------------------+------------------+-------------------+-------------------+-------------------+----+\n",
      "|               em_0|                em_1|                em_2|                em_3|               em_4|                em_5|              em_6|               em_7|               em_8|                em_9|             em_10|              em_11|              em_12|              em_13|  id|\n",
      "+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+-------------------+--------------------+------------------+-------------------+-------------------+-------------------+----+\n",
      "|-0.5829833149909973| -0.9786504507064819|-0.13974636793136597|-0.08100467175245285|-0.7230328321456909| -0.6167192459106445|0.3849627375602722|-0.8253325819969177|-0.6468784809112549| 0.09811028093099594|0.9364645481109619|-0.7005855441093445|0.48287415504455566| 1.1603587865829468|7605|\n",
      "|-1.0027698278427124|-0.18664376437664032| -0.6993895173072815| -0.8623955249786377|-0.4360705316066742|-0.05579366162419319| 0.878957986831665| -0.326903373003006| 0.9595316648483276|-0.30835017561912537|0.0633615255355835|0.28870928287506104|-0.4725606441497803|-0.6981539726257324|6075|\n",
      "+-------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+-------------------+--------------------+------------------+-------------------+-------------------+-------------------+----+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "node_embeddings_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hops import hdfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve nodes training dataset from hsfs to get label infamation, whether node was part of the previously known money laundering scheme or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, coalesce, concat,  col\n",
    "\n",
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "node_labels_df = node_td.read().select(\"id\",\"is_sar\")\n",
    "#node_labels_df = node_labels_df.select(*(col(c).cast(\"float\").alias(c) for c in node_labels_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|is_sar|\n",
      "+----+------+\n",
      "|7605|     1|\n",
      "|6075|     1|\n",
      "|5814|     1|\n",
      "|9360|     1|\n",
      "|4626|     1|\n",
      "+----+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "node_labels_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_df = node_embeddings_df.join(node_labels_df,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998"
     ]
    }
   ],
   "source": [
    "node_embeddings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_td = node_embeddings_df.drop(\"id\").withColumn(\"embedding\", array(emb_feature_names)).select(\"is_sar\",\"embedding\").withColumnRenamed(\"is_sar\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|           embedding|\n",
      "+------+--------------------+\n",
      "|     1|[-0.5829833149909...|\n",
      "|     1|[-1.0027698278427...|\n",
      "|     1|[-0.4906578361988...|\n",
      "|     1|[-0.2083134651184...|\n",
      "|     1|[-0.7868499755859...|\n",
      "|     1|[-0.5728423595428...|\n",
      "|     1|[-0.2061972171068...|\n",
      "|     1|[0.42000821232795...|\n",
      "|     1|[1.26069307327270...|\n",
      "|     0|[0.17879278957843...|\n",
      "|     0|[-1.0163079500198...|\n",
      "|     0|[0.25647121667861...|\n",
      "|     0|[-0.9605649709701...|\n",
      "|     0|[-0.6518237590789...|\n",
      "|     0|[-0.7031417489051...|\n",
      "|     0|[-0.0500299446284...|\n",
      "|     0|[0.07430136948823...|\n",
      "|     0|[0.28442481160163...|\n",
      "|     0|[0.15200684964656...|\n",
      "|     0|[1.22951471805572...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "emb_td.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training datasets for anomaly detection \n",
    "###### In the next notebook we are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for money laundering behaviour.  But we will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_df = emb_td.where(col(\"target\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_df = emb_td.where(col(\"target\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "[non_sar_train_df, non_sar_eval_df] = non_sar_df.randomSplit([0.8, 0.02],seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fe08cfade50>"
     ]
    }
   ],
   "source": [
    "non_sar_td = fs.create_training_dataset(name=\"gan_non_sar_training_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"non sar dataset for gan training\")\n",
    "non_sar_td.save(non_sar_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = non_sar_eval_df.union(sar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fe08cfc88d0>"
     ]
    }
   ],
   "source": [
    "gan_eval_ds = fs.create_training_dataset(name=\"gan_eval_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"evaluation dataset for gan training\")\n",
    "gan_eval_ds.save(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
