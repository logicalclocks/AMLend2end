{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>7</td><td>application_1607026686089_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607026686089_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-112.us-west-2.compute.internal:8042/node/containerlogs/container_e02_1607026686089_0008_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f4860c25790>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stellargraph import StellarGraph, datasets, StellarDiGraph\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_fg = fs.get_feature_group(\"node_features\", 1)\n",
    "edge_fg = fs.get_feature_group(\"edge_features\", 1)\n",
    "source = edge_fg.select([\"source\",\"is_sar\"]).read().withColumnRenamed(\"source\",\"id\")\n",
    "target = edge_fg.select([\"target\",\"is_sar\"]).read().withColumnRenamed(\"target\",\"id\")\n",
    "nodes = source.union(target)\n",
    "ano_nodes = nodes.where(nodes.is_sar == 1)\n",
    "ben = nodes.where(nodes.is_sar == 0)\n",
    "ben_nodes = ben.join(ano_nodes, [\"id\"], \"leftanti\")\n",
    "labels = ano_nodes.union(ben_nodes).dropDuplicates(subset=[\"id\"])\n",
    "labels.coalesce(1)\\\n",
    "      .write\\\n",
    "      .option(\"header\",\"true\")\\\n",
    "      .option(\"sep\",\",\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .csv(\"hdfs:///Projects/amlsim/Resources/node_labels_for_plotting.csv\")\n",
    "# Get fg as pandas\n",
    "node_pdf = node_fg.read().toPandas()\n",
    "edge_pdf = edge_fg.read().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining StellarDiGraph"
     ]
    }
   ],
   "source": [
    "print('Defining StellarDiGraph')\n",
    "node_data = pd.DataFrame(node_pdf[['tx_behavior_id','prior_sar','initial_deposit','gender','age']], index=node_pdf['acct_id'])\n",
    "G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "graph = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarDiGraph: Directed multigraph\n",
      " Nodes: 10000, Edges: 1029696\n",
      "\n",
      " Node types:\n",
      "  default: [10000]\n",
      "    Features: float32 vector, length 5\n",
      "    Edge types: default-4->default\n",
      "\n",
      " Edge types:\n",
      "    default-4->default: [1029696]\n",
      "        Weights: all 1 (default)\n",
      "        Features: float32 vector, length 5"
     ]
    }
   ],
   "source": [
    "print(graph.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 102969 positive and 102969 negative edges. **\n",
      "StellarDiGraph: Directed multigraph\n",
      " Nodes: 10000, Edges: 926727\n",
      "\n",
      " Node types:\n",
      "  default: [10000]\n",
      "    Features: float32 vector, length 5\n",
      "    Edge types: default-4->default\n",
      "\n",
      " Edge types:\n",
      "    default-4->default: [926727]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none"
     ]
    }
   ],
   "source": [
    "# Define an edge splitter on the original graph:\n",
    "edge_splitter_test = EdgeSplitter(graph)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from graph, and obtain the\n",
    "# reduced graph graph_test with the sampled links removed:\n",
    "graph_test, examples_test, labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\"\n",
    ")\n",
    "\n",
    "print(graph_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 92672 positive and 92672 negative edges. **\n",
      "StellarDiGraph: Directed multigraph\n",
      " Nodes: 10000, Edges: 834055\n",
      "\n",
      " Node types:\n",
      "  default: [10000]\n",
      "    Features: float32 vector, length 5\n",
      "    Edge types: default-4->default\n",
      "\n",
      " Edge types:\n",
      "    default-4->default: [834055]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none"
     ]
    }
   ],
   "source": [
    "# Do the same process to compute a training subset from within the test graph\n",
    "edge_splitter_train = EdgeSplitter(graph_test)\n",
    "graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method=\"global\"\n",
    ")\n",
    "(\n",
    "    examples_train,\n",
    "    examples_model_selection,\n",
    "    labels_train,\n",
    "    labels_model_selection,\n",
    ") = train_test_split(examples, labels, train_size=0.75, test_size=0.25)\n",
    "\n",
    "print(graph_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the different splits that have been created in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Number of Examples  ...                                    Use\n",
      "Split                                ...                                       \n",
      "Training Set                 139008  ...              Train the Link Classifier\n",
      "Model Selection               46336  ...  Select the best Link Classifier model\n",
      "Test set                     205938  ...      Evaluate the best Link Classifier\n",
      "\n",
      "[3 rows x 4 columns]"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"Training Set\",\n",
    "            len(examples_train),\n",
    "            \"Train Graph\",\n",
    "            \"Test Graph\",\n",
    "            \"Train the Link Classifier\",\n",
    "        ),\n",
    "        (\n",
    "            \"Model Selection\",\n",
    "            len(examples_model_selection),\n",
    "            \"Train Graph\",\n",
    "            \"Test Graph\",\n",
    "            \"Select the best Link Classifier model\",\n",
    "        ),\n",
    "        (\n",
    "            \"Test set\",\n",
    "            len(examples_test),\n",
    "            \"Test Graph\",\n",
    "            \"Full Graph\",\n",
    "            \"Evaluate the best Link Classifier\",\n",
    "        ),\n",
    "    ],\n",
    "    columns=(\"Split\", \"Number of Examples\", \"Hidden from\", \"Picked from\", \"Use\"),\n",
    ").set_index(\"Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import BiasedRandomWalk\n",
    "\n",
    "\n",
    "def create_biased_random_walker(graph, walk_num, walk_length):\n",
    "    # parameter settings for \"p\" and \"q\":\n",
    "    p = 1.0\n",
    "    q = 1.0\n",
    "    return BiasedRandomWalk(graph, n=walk_num, length=walk_length, p=p, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import UnsupervisedSampler\n",
    "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "from stellargraph.layer import Node2Vec, link_classification\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def node2vec_embedding(graph, name):\n",
    "\n",
    "    # Set the embedding dimension and walk number:\n",
    "    dimension = 128\n",
    "    walk_number = 20\n",
    "\n",
    "    print(f\"Training Node2Vec for '{name}':\")\n",
    "\n",
    "    graph_node_list = list(graph.nodes())\n",
    "\n",
    "    # Create the biased random walker to generate random walks\n",
    "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
    "\n",
    "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
    "    unsupervised_samples = UnsupervisedSampler(\n",
    "        graph, nodes=graph_node_list, walker=walker\n",
    "    )\n",
    "\n",
    "    # Define a Node2Vec training generator, which generates batches of training pairs\n",
    "    generator = Node2VecLinkGenerator(graph, batch_size)\n",
    "\n",
    "    # Create the Node2Vec model\n",
    "    node2vec = Node2Vec(dimension, generator=generator)\n",
    "\n",
    "    # Build the model and expose input and output sockets of Node2Vec, for node pair inputs\n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "\n",
    "    # Use the link_classification function to generate the output of the Node2Vec model\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    # Stack the Node2Vec encoder and prediction layer into a Keras model, and specify the loss\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Build the model to predict node representations from node ids with the learned Node2Vec model parameters\n",
    "    x_inp_src = x_inp[0]\n",
    "    x_out_src = x_out[0]\n",
    "    embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)\n",
    "\n",
    "    # Get representations for all nodes in ``graph``\n",
    "    node_gen = Node2VecNodeGenerator(graph, batch_size).flow(graph_node_list)\n",
    "    node_embeddings = embedding_model.predict(node_gen, workers=1, verbose=0)\n",
    "\n",
    "    def get_embedding(u):\n",
    "        u_index = graph_node_list.index(u)\n",
    "        return node_embeddings[u_index]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import FullBatchLinkGenerator, FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN, LinkEmbedding\n",
    "\n",
    "\n",
    "def gcn_embedding(graph, name):\n",
    "\n",
    "    # Set the embedding dimensions and walk number:\n",
    "    dimensions = [128, 128]\n",
    "    walk_number = 1\n",
    "\n",
    "    print(f\"Training GCN for '{name}':\")\n",
    "\n",
    "    graph_node_list = list(graph.nodes())\n",
    "\n",
    "    # Create the biased random walker to generate random walks\n",
    "    walker = create_biased_random_walker(graph, walk_number, walk_length)\n",
    "\n",
    "    # Create the unsupervised sampler to sample (target, context) pairs from random walks\n",
    "    unsupervised_samples = UnsupervisedSampler(\n",
    "        graph, nodes=graph_node_list, walker=walker\n",
    "    )\n",
    "\n",
    "    # Define a GCN training generator, which generates the full batch of training pairs\n",
    "    generator = FullBatchLinkGenerator(graph, method=\"gcn\")\n",
    "\n",
    "    # Create the GCN model\n",
    "    gcn = GCN(\n",
    "        layer_sizes=dimensions,\n",
    "        activations=[\"relu\", \"relu\"],\n",
    "        generator=generator,\n",
    "        dropout=0.3,\n",
    "    )\n",
    "\n",
    "    # Build the model and expose input and output sockets of GCN, for node pair inputs\n",
    "    x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "    # Use the dot product of node embeddings to make node pairs co-occurring in short random walks represented closely\n",
    "    prediction = LinkEmbedding(activation=\"sigmoid\", method=\"ip\")(x_out)\n",
    "    prediction = keras.layers.Reshape((-1,))(prediction)\n",
    "\n",
    "    # Stack the GCN encoder and prediction layer into a Keras model, and specify the loss\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    batches = unsupervised_samples.run(batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        batch_iter = 1\n",
    "        for batch in batches:\n",
    "            samples = generator.flow(batch[0], targets=batch[1], use_ilocs=True)[0]\n",
    "            [loss, accuracy] = model.train_on_batch(x=samples[0], y=samples[1])\n",
    "            output = (\n",
    "                f\"{batch_iter}/{len(batches)} - loss:\"\n",
    "                + \" {:6.4f}\".format(loss)\n",
    "                + \" - binary_accuracy:\"\n",
    "                + \" {:6.4f}\".format(accuracy)\n",
    "            )\n",
    "            if batch_iter == len(batches):\n",
    "                print(output)\n",
    "            else:\n",
    "                print(output, end=\"\\r\")\n",
    "            batch_iter = batch_iter + 1\n",
    "\n",
    "    # Get representations for all nodes in ``graph``\n",
    "    embedding_model = keras.Model(inputs=x_inp, outputs=x_out)\n",
    "    node_embeddings = embedding_model.predict(\n",
    "        generator.flow(list(zip(graph_node_list, graph_node_list)))\n",
    "    )\n",
    "    node_embeddings = node_embeddings[0][:, 0, :]\n",
    "\n",
    "    def get_embedding(u):\n",
    "        u_index = graph_node_list.index(u)\n",
    "        return node_embeddings[u_index]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 1. link embeddings\n",
    "def link_examples_to_features(link_examples, transform_node, binary_operator):\n",
    "    return [\n",
    "        binary_operator(transform_node(src), transform_node(dst))\n",
    "        for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "\n",
    "# 2. training classifier\n",
    "def train_link_prediction_model(\n",
    "    link_examples, link_labels, get_embedding, binary_operator\n",
    "):\n",
    "    clf = link_prediction_classifier()\n",
    "    link_features = link_examples_to_features(\n",
    "        link_examples, get_embedding, binary_operator\n",
    "    )\n",
    "    clf.fit(link_features, link_labels)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def link_prediction_classifier(max_iter=5000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter)\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n",
    "\n",
    "\n",
    "# 3. and 4. evaluate classifier\n",
    "def evaluate_link_prediction_model(\n",
    "    clf, link_examples_test, link_labels_test, get_embedding, binary_operator\n",
    "):\n",
    "    link_features_test = link_examples_to_features(\n",
    "        link_examples_test, get_embedding, binary_operator\n",
    "    )\n",
    "    score = evaluate_roc_auc(clf, link_features_test, link_labels_test)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_roc_auc(clf, link_features, link_labels):\n",
    "    predicted = clf.predict_proba(link_features)\n",
    "\n",
    "    # check which class corresponds to positive links\n",
    "    positive_column = list(clf.classes_).index(1)\n",
    "    return roc_auc_score(link_labels, predicted[:, positive_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_hadamard(u, v):\n",
    "    return u * v\n",
    "\n",
    "\n",
    "def operator_l1(u, v):\n",
    "    return np.abs(u - v)\n",
    "\n",
    "\n",
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "\n",
    "def operator_avg(u, v):\n",
    "    return (u + v) / 2.0\n",
    "\n",
    "\n",
    "def run_link_prediction(binary_operator, embedding_train):\n",
    "    clf = train_link_prediction_model(\n",
    "        examples_train, labels_train, embedding_train, binary_operator\n",
    "    )\n",
    "    score = evaluate_link_prediction_model(\n",
    "        clf,\n",
    "        examples_model_selection,\n",
    "        labels_model_selection,\n",
    "        embedding_train,\n",
    "        binary_operator,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"classifier\": clf,\n",
    "        \"binary_operator\": binary_operator,\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "\n",
    "binary_operators = [operator_hadamard, operator_l1, operator_l2, operator_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(embedding, name):\n",
    "\n",
    "    embedding_train = embedding(graph_train, \"Train Graph\")\n",
    "\n",
    "    # Train the link classification model with the learned embedding\n",
    "    results = [run_link_prediction(op, embedding_train) for op in binary_operators]\n",
    "    best_result = max(results, key=lambda result: result[\"score\"])\n",
    "    print(\n",
    "        f\"\\nBest result with '{name}' embeddings from '{best_result['binary_operator'].__name__}'\"\n",
    "    )\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            [(result[\"binary_operator\"].__name__, result[\"score\"]) for result in results],\n",
    "            columns=(\"name\", \"ROC AUC\"),\n",
    "        ).set_index(\"name\")\n",
    "    )\n",
    "\n",
    "    # Evaluate the best model using the test set\n",
    "    test_score = evaluate_link_prediction_model(\n",
    "        best_result[\"classifier\"],\n",
    "        examples_test,\n",
    "        labels_test,\n",
    "        embedding_train,\n",
    "        best_result[\"binary_operator\"],\n",
    "    )\n",
    "\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Node2Vec for 'Train Graph':\n",
      "link_classification: using 'dot' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/6\n",
      "31994/31994 - 216s - loss: 0.6219 - binary_accuracy: 0.6223\n",
      "Epoch 2/6\n",
      "31994/31994 - 213s - loss: 0.5946 - binary_accuracy: 0.6710\n",
      "Epoch 3/6\n",
      "31994/31994 - 212s - loss: 0.5471 - binary_accuracy: 0.7171\n",
      "Epoch 4/6\n",
      "31994/31994 - 211s - loss: 0.4704 - binary_accuracy: 0.7834\n",
      "Epoch 5/6\n",
      "31994/31994 - 214s - loss: 0.4445 - binary_accuracy: 0.8105\n",
      "Epoch 6/6\n",
      "31994/31994 - 213s - loss: 0.4314 - binary_accuracy: 0.8238\n",
      "\n",
      "Best result with 'Node2Vec' embeddings from 'operator_l2'\n",
      "                    ROC AUC\n",
      "name                       \n",
      "operator_hadamard  0.949846\n",
      "operator_l1        0.965042\n",
      "operator_l2        0.973594\n",
      "operator_avg       0.516817"
     ]
    }
   ],
   "source": [
    "node2vec_result = train_and_evaluate(node2vec_embedding, \"Node2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN for 'Train Graph':\n",
      "Using GCN (local pooling) filters...\n",
      "Epoch: 1/6\n",
      "1600/1600 - loss: 8.5216 - binary_accuracy: 0.44120\n",
      "Epoch: 2/6\n",
      "1600/1600 - loss: 8.5216 - binary_accuracy: 0.44120\n",
      "Epoch: 3/6\n",
      "1600/1600 - loss: 8.5216 - binary_accuracy: 0.44120\n",
      "Epoch: 4/6\n",
      "1600/1600 - loss: 8.5216 - binary_accuracy: 0.44120\n",
      "Epoch: 5/6\n",
      "1600/1600 - loss: 8.5216 - binary_accuracy: 0.44120\n",
      "Epoch: 6/6\n",
      "1600/1600 - loss: 8.5216 - binary_accuracy: 0.44120\n",
      "\n",
      "Best result with 'GCN' embeddings from 'operator_avg'\n",
      "                    ROC AUC\n",
      "name                       \n",
      "operator_hadamard  0.628396\n",
      "operator_l1        0.554716\n",
      "operator_l2        0.551172\n",
      "operator_avg       0.629564"
     ]
    }
   ],
   "source": [
    "gcn_result = train_and_evaluate(gcn_embedding, \"GCN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ROC AUC\n",
      "name              \n",
      "Node2Vec  0.973998\n",
      "GCN       0.628841"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        (\"Node2Vec\", node2vec_result),\n",
    "        (\"GCN\", gcn_result),\n",
    "    ],\n",
    "    columns=(\"name\", \"ROC AUC\"),\n",
    ").set_index(\"name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
