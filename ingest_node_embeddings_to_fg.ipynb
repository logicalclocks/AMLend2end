{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Model Repository for best node embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>7</td><td>application_1607892309705_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607892309705_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-109.us-west-2.compute.internal:8042/node/containerlogs/container_e02_1607892309705_0008_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"NodeEmbeddings\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'application_1607892309705_0005_1'"
     ]
    }
   ],
   "source": [
    "best_model['experimentId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and load wights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "\n",
    "import pandas as pd\n",
    "from stellargraph import StellarDiGraph\n",
    "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "from stellargraph.data import UnsupervisedSampler, BiasedRandomWalk\n",
    "from stellargraph.layer import Node2Vec\n",
    "import pydoop.hdfs as pydoop\n",
    "import hsfs\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = pd.DataFrame(node_pdf[['tx_behavior_id','prior_sar','initial_deposit']], index=node_pdf['id'])\n",
    "\n",
    "print('Defining StellarDiGraph')\n",
    "G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json\n",
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_number = 2\n",
    "walk_length = 2\n",
    "batch_size = 1\n",
    "emb_size = args_dict['emb_size'][0]\n",
    "# Extracting node embeddings\n",
    "walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "generator = Node2VecLinkGenerator(G, batch_size)\n",
    "\n",
    "node2vec = Node2Vec(emb_size, generator=generator)\n",
    "x_inp, x_out = node2vec.in_out_tensors()\n",
    "\n",
    "x_inp_src = x_inp[0]\n",
    "x_out_src = x_out[0]\n",
    "embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(\"hdfs:///Projects/{}/Experiments/\".format(hdfs.project_name()) + best_model['experimentId'])\n",
    "embedding_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "node_gen = Node2VecNodeGenerator(G, batch_size).flow(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "pdf = pd.DataFrame(embedding_model.predict(node_gen), index=G.nodes())\n",
    "emb_feature_names = [\"em_\" + str(c)  for c in pdf.columns]\n",
    "pdf.columns = emb_feature_names\n",
    "pdf['id'] = pdf.index\n",
    "node_embeddings_df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "from hops import hdfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nodes training dataset from hsfs to get label infamation, whether node was part of the previously known money laundering scheme or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, coalesce, concat,  col\n",
    "\n",
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "node_labels_df = node_td.read().select(\"id\",\"is_sar\")\n",
    "#node_labels_df = node_labels_df.select(*(col(c).cast(\"float\").alias(c) for c in node_labels_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_df = node_embeddings_df.join(node_labels_df,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_td = node_embeddings_df.drop(\"id\").withColumn(\"embedding\", array(emb_feature_names)).select(\"is_sar\",\"embedding\").withColumnRenamed(\"is_sar\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_td.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training datasets for anomaly detection \n",
    "###### In the next notebook we are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for money laundering behaviour.  But we will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_df = emb_td.where(col(\"target\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_df = emb_td.where(col(\"target\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "[non_sar_train_df, non_sar_eval_df] = non_sar_df.randomSplit([0.8, 0.02],seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_td = fs.create_training_dataset(name=\"gan_non_sar_training_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"non sar dataset for gan training\")\n",
    "non_sar_td.save(non_sar_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = non_sar_eval_df.union(sar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_eval_ds = fs.create_training_dataset(name=\"gan_eval_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"evaluation dataset for gan training\")\n",
    "gan_eval_ds.save(eval_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
