{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>40</td><td>application_1607211657348_0042</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607211657348_0042/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1607211657348_0042_01_000001/amlsim2__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fcd0f581f10>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, coalesce, concat,  col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hops import hdfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nodes training dataset from hsfs to get label infamation, whether node was part of the previously known money laundering scheme or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "node_labels_df = node_td.read().select(\"id\",\"is_sar\")\n",
    "node_labels_df = node_labels_df.select(*(col(c).cast(\"float\").alias(c) for c in node_labels_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    id|is_sar|\n",
      "+------+------+\n",
      "|7605.0|   1.0|\n",
      "|6075.0|   1.0|\n",
      "|5814.0|   1.0|\n",
      "|9360.0|   1.0|\n",
      "|4626.0|   1.0|\n",
      "+------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "node_labels_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read computed node embeddings data and concatenate it as `array<float>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_df = spark.read.csv(\"hdfs:///Projects/{}/Resources/node_embeddings_features_2_2_16.csv\".format(hdfs.project_name(),inferSchema =True,header=False))\n",
    "node_embeddings_df = node_embeddings_df.select(*(col(c).cast(\"float\").alias(c) for c in node_embeddings_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: float (nullable = true)\n",
      " |-- _c1: float (nullable = true)\n",
      " |-- _c2: float (nullable = true)\n",
      " |-- _c3: float (nullable = true)\n",
      " |-- _c4: float (nullable = true)\n",
      " |-- _c5: float (nullable = true)\n",
      " |-- _c6: float (nullable = true)\n",
      " |-- _c7: float (nullable = true)\n",
      " |-- _c8: float (nullable = true)\n",
      " |-- _c9: float (nullable = true)\n",
      " |-- _c10: float (nullable = true)\n",
      " |-- _c11: float (nullable = true)\n",
      " |-- _c12: float (nullable = true)\n",
      " |-- _c13: float (nullable = true)\n",
      " |-- _c14: float (nullable = true)\n",
      " |-- _c15: float (nullable = true)\n",
      " |-- _c16: float (nullable = true)"
     ]
    }
   ],
   "source": [
    "node_embeddings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = node_embeddings_df.columns\n",
    "feature_names = [\"_\" + s + \"c\"  for s in feature_names]\n",
    "feature_names[0]= 'id'\n",
    "node_embeddings_node_embeddings_df = node_embeddings_df.toDF(*feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.remove('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_td = node_embeddings_node_embeddings_df.join(node_labels_df, ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_td = emb_td.drop(\"id\").withColumn(\"embedding\", array(feature_names)).select(\"is_sar\",\"embedding\").withColumnRenamed(\"is_sar\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: float (nullable = true)\n",
      " |-- embedding: array (nullable = false)\n",
      " |    |-- element: float (containsNull = true)"
     ]
    }
   ],
   "source": [
    "emb_td.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training datasets for anomaly detection \n",
    "###### In the next notebook we are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for money laundering behaviour.  But we will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_df = emb_td.where(col(\"target\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_df = emb_td.where(col(\"target\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "[non_sar_train_df, non_sar_eval_df] = non_sar_df.randomSplit([0.8, 0.02],seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fcd1ef15b50>"
     ]
    }
   ],
   "source": [
    "non_sar_td = fs.create_training_dataset(name=\"gan_non_sar_training_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"non sar dataset for gan training\")\n",
    "non_sar_td.save(non_sar_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets prepare dataset for evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = non_sar_eval_df.union(sar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fcd1ef59310>"
     ]
    }
   ],
   "source": [
    "gan_eval_ds = fs.create_training_dataset(name=\"gan_eval_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"evaluation dataset for gan training\")\n",
    "gan_eval_ds.save(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
