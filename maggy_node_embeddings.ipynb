{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The Spark session does not have enough YARN resources to start. \n",
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nodes and edges training datasets from hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training dataset in to pandas daframe\n",
    "We are going to use StellarGraph library to compute node embeddings. StellarGraph supports loading data via Pandas DataFrames, NumPy arrays, Neo4j and NetworkX graphs. \n",
    "\n",
    "---\n",
    "**NOTE**:\n",
    "\n",
    "Loading large scale dataset in to StellarGraph for training can not be handled with above mentioned fameworks. It will require loading data using frameworks such as `tf.data`. \n",
    "\n",
    "If your training datasets measure from couple of GB to 100s of GBs or even TBs contact us at Logical Clocks and we will help you to setup distributed training pipelines. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The searchspace can be instantiated with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "sp = Searchspace(walk_number=('INTEGER', [2, 3]), walk_length=('INTEGER', [2, 3]) , emb_size=('INTEGER', [12, 16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "def embeddings_computer(walk_number, walk_length, emb_size):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import pydoop.hdfs as pydoop\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from hops import hdfs\n",
    "    from hops import pandas_helper as pandas\n",
    "    from hops import model as hops_model\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import preprocessing, feature_extraction, model_selection\n",
    "    from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    import stellargraph as sg\n",
    "    from stellargraph import StellarGraph\n",
    "    from stellargraph import StellarDiGraph\n",
    "    from stellargraph.data import BiasedRandomWalk\n",
    "    from stellargraph.data import UnsupervisedSampler\n",
    "    from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "    from stellargraph.layer import Node2Vec, link_classification\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras  \n",
    "        \n",
    "    ###########\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    num_samples = [20, 20]\n",
    "    layer_sizes = [100, 100]\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    node_data = pd.DataFrame(node_pdf[['tx_behavior_id','prior_sar','initial_deposit']], index=node_pdf['id'])\n",
    "    ###########\n",
    "        \n",
    "    print('Defining StellarDiGraph')\n",
    "    G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "    unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "    generator = Node2VecLinkGenerator(G, batch_size)\n",
    "    node2vec = Node2Vec(emb_size, generator=generator)\n",
    "    \n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    print('Defining the model')\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "        \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    \n",
    "    print('Training the model')\n",
    "    history = model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    binary_accuracy = history.history['binary_accuracy'][-1]\n",
    "        \n",
    "    return binary_accuracy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct maggy hyperparameter search experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "result = experiment.lagom(embeddings_computer, \n",
    "                           searchspace=sp, \n",
    "                           optimizer='randomsearch', \n",
    "                           direction='max',\n",
    "                           num_trials=2, \n",
    "                           name='EMBEDDINGS',\n",
    "                           hb_interval=5, \n",
    "                           es_interval=5,\n",
    "                           es_min=5\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 28 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2020-12-15 09:32:04,230 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-12-15 09:32:04,328 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2020-12-15 09:32:04,454 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2020-12-15 09:32:05,343 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2020-12-15 09:32:05,437 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2020-12-15 09:32:05,449 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2020-12-15 09:32:05,473 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2020-12-15 09:32:05,474 INFO  Client: Will allocate AM container, with 4452 MB memory including 404 MB overhead\n",
      "2020-12-15 09:32:05,474 INFO  Client: Setting up container launch context for our AM\n",
      "2020-12-15 09:32:05,479 INFO  Client: Setting up the launch environment for our AM container\n",
      "2020-12-15 09:32:05,494 INFO  Client: Preparing resources for our AM container\n",
      "2020-12-15 09:32:06,323 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2020-12-15 09:32:06,429 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2020-12-15 09:32:06,438 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2020-12-15 09:32:06,448 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2020-12-15 09:32:06,627 INFO  Client: Uploading resource file:/tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509/__spark_conf__7799456930926252769.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1607949680860_0030/__spark_conf__.zip\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,133 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing view acls groups to: \n",
      "2020-12-15 09:32:07,134 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2020-12-15 09:32:07,135 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2020-12-15 09:32:07,207 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2020-12-15 09:32:08,439 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2020-12-15 09:32:08,450 INFO  Client: Submitting application application_1607949680860_0030 to ResourceManager\n",
      "2020-12-15 09:32:08,533 INFO  YarnClientImpl: Submitted application application_1607949680860_0030\n",
      "2020-12-15 09:32:08,539 INFO  Client: Application report for application_1607949680860_0030 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2020-12-15 09:32:08,545 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1608024728477\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0030/\n",
      "\t user: amlsim__meb10179\n",
      "2020-12-15 09:32:08,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2020-12-15 09:32:08,558 INFO  ShutdownHookManager: Deleting directory /tmp/spark-26646314-31b9-40c3-970a-6d66420945c4\n",
      "2020-12-15 09:32:08,567 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3121f0ae-a717-452d-af14-bcebc193a509\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1607949680860_0030 was killed by user livy at 10.0.0.4.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from hops import hdfs\n",
    "EMBEDDINGS_HYPERPARAMS_FILE = 'embeddings_best_hp.json'\n",
    "hdfs.dump(json.dumps(result['best_hp']), \"Resources/\" + EMBEDDINGS_HYPERPARAMS_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
