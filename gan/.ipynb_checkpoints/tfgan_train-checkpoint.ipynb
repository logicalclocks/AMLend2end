{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>20</td><td>application_1606908943181_0021</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1606908943181_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://riccardoaml-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e02_1606908943181_0021_01_000001/gan_test__davit000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f03fc3e1ed0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019 The TensorFlow GAN Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Trains a GANEstimator on MNIST data using `train_and_evaluate`.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_gan.examples.anomaly import train_gan_enc_experiment_lib\n",
    "import pydoop.hdfs as pydoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_main(\n",
    "\n",
    "             gp_w = 1,   # gp_weight\n",
    "             d_n = 2,    # d_n_neurons\n",
    "             d_n_l = 1,  # d_n_layers\n",
    "             d_a_f = 1,  # d_activation_fn\n",
    "             d_a_d_b = 1,  # d_double_neurons or d_bottlneck_neurons\n",
    "             d_b_n = 0,  # d_batch_norm\n",
    "             d_dr = 0,   # d_batch_dropout\n",
    "             d_b_r = 1,  # d_dropout_rate\n",
    "             d_lr = 1,   # d_learning_rate\n",
    "             d_k_b_r = 1, # d_kernel_bias_reg\n",
    "             d_l1_r = 1, # int_to_l1_rate\n",
    "             d_l2_r = 1, # int_to_l2_rate\n",
    "\n",
    "             g_n = 5,    # g_n_neurons\n",
    "             g_n_l = 1,  # g_n_layers\n",
    "             g_a_f = 1,  # g_activation_fn\n",
    "             g_a_d_b = 1,  # g_double_neurons or g_bottlneck_neurons\n",
    "             g_b_n = 0,  # g_batch_norm\n",
    "             g_dr = 0,   # g_batch_dropout\n",
    "             g_b_r =  1,  # g_dropout_rate\n",
    "             g_k_b_r = 1, # g_kernel_bias_reg\n",
    "             g_lr =  1,    # generator_lr\n",
    "             g_l1_r = 1, #g_l1_rate\n",
    "             g_l2_r = 1, #g_l2_rate\n",
    "\n",
    "             en_hp = 1  # encoder_n_layers        \n",
    "\n",
    "\n",
    "):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "    disable_eager_execution()\n",
    "    \n",
    "    #################\n",
    "    # Name cannot be empty, cannot exceed 256 characters and must match the regular expression: ^[a-zA-Z0-9]+$\n",
    "    model_name = 'ganomaly' \n",
    "    ################# \n",
    "    \n",
    "    int_to_neurons = {\n",
    "        1: 4096,\n",
    "        2: 2048,\n",
    "        3: 1024,\n",
    "        4: 512, \n",
    "        5: 256, \n",
    "        6: 128,\n",
    "        7: 64,\n",
    "        8: 32\n",
    "    }\n",
    "\n",
    "    int_to_learning_rate = {\n",
    "        1: 0.00001,\n",
    "        2: 0.001,\n",
    "        3: 0.0015,\n",
    "        4: 0.002,\n",
    "        5: 0.0025,\n",
    "        6: 0.003,\n",
    "        7: 0.0035,\n",
    "        8: 0.004,\n",
    "        9: 0.0045,\n",
    "        10: 0.005,\n",
    "        11: 0.0055,\n",
    "        12: 0.006,\n",
    "        13: 0.0065,\n",
    "        14: 0.007,\n",
    "        15: 0.0075,\n",
    "        16: 0.008,\n",
    "        17: 0.0085,\n",
    "        18: 0.009,\n",
    "        19: 0.0095,\n",
    "        20: 0.01, \n",
    "        21: 0.02, \n",
    "        22: 0.03, \n",
    "        23: 0.04 \n",
    "    }  \n",
    "    \n",
    "    int_to_dropout_rate= {\n",
    "        1: 0.1,\n",
    "        2: 0.15,\n",
    "        3: 0.2,\n",
    "        4: 0.25,\n",
    "        5: 0.3,\n",
    "        6: 0.35,\n",
    "        7: 0.4,\n",
    "        8: 0.45,\n",
    "        9: 0.5,\n",
    "        10: 0.55,\n",
    "        11: 0.6 \n",
    "    }\n",
    "    \n",
    "    int_to_activation_fn= {\n",
    "        1: 'relu',\n",
    "        2: 'leaky_relu',\n",
    "        3: 'tanh',\n",
    "        4: 'selu',\n",
    "        5: 'linear'\n",
    "    }\n",
    "    \n",
    "    int_to_l1_rate = {\n",
    "        1: 0.0001,\n",
    "        2: 0.0002,\n",
    "        3: 0.0003,\n",
    "        4: 0.0004,\n",
    "        5: 0.0005,\n",
    "        6: 0.0006,\n",
    "        7: 0.0007,\n",
    "        8: 0.0008,\n",
    "        9: 0.0009,\n",
    "        10: 0.001,\n",
    "        11: 0.002\n",
    "    }\n",
    "    \n",
    "    int_to_l2_rate = {\n",
    "        1: 0.4,\n",
    "        2: 0.45,\n",
    "        3: 0.5,\n",
    "        4: 0.55,\n",
    "        5: 0.6, \n",
    "        6: 0.65, \n",
    "        7: 0.7, \n",
    "        8: 0.75, \n",
    "        9: 0.8, \n",
    "        10: 0.9, \n",
    "        11: 0.95\n",
    "    }\n",
    "\n",
    "    int_to_gp_weight = {\n",
    "        1: 1.0,\n",
    "        2: 5.0,\n",
    "        3: 10.0\n",
    "    }\n",
    "    \n",
    "    d_arch_double_bottlneck = d_a_d_b\n",
    "    g_arch_double_bottlneck = g_a_d_b\n",
    "    \n",
    "    \n",
    "    # ML Hparams.\n",
    "    n_epochs=1\n",
    "    data_size=300\n",
    "    batch_size=16\n",
    "    #noise_dims=64 #int(int(int_to_neurons[g_n]/2)/2)\n",
    "    g_output_dim=365\n",
    "    d_output_dim=1\n",
    "    feature_dim=1\n",
    "    time_steps=365\n",
    "    timeseries=False\n",
    "\n",
    "    gp_weight=int_to_gp_weight[gp_w]\n",
    "\n",
    "    d_n_neurons=int_to_neurons[d_n]\n",
    "    d_n_layers=d_n_l\n",
    "    d_activation_fn=int_to_activation_fn[d_a_f]\n",
    "    if d_arch_double_bottlneck == 1: \n",
    "        d_double_neurons = True\n",
    "        d_bottlneck_neurons= False\n",
    "    elif d_arch_double_bottlneck == 2: \n",
    "        d_double_neurons = False\n",
    "        d_bottlneck_neurons= True\n",
    "    elif d_arch_double_bottlneck == 3: \n",
    "        d_double_neurons = False\n",
    "        d_bottlneck_neurons= False\n",
    "    \n",
    "    if d_b_n == 0:     \n",
    "        d_batch_norm=False\n",
    "    else:\n",
    "        d_batch_norm=True\n",
    "    \n",
    "    if d_dr == 0:\n",
    "        d_batch_dropout=False\n",
    "    else:     \n",
    "        d_batch_dropout=True\n",
    "        \n",
    "    d_dropout_rate=int_to_dropout_rate[d_b_r]\n",
    "    d_kernel_bias_reg=d_k_b_r\n",
    "    discriminator_lr=int_to_learning_rate[d_lr]\n",
    "    d_l1_rate=int_to_l1_rate[d_l1_r]\n",
    "    d_l2_rate=int_to_l2_rate[d_l2_r]\n",
    "\n",
    "    g_n_neurons=int_to_neurons[g_n]\n",
    "    g_n_layers=g_n_l\n",
    "    g_activation_fn=int_to_activation_fn[g_a_f]\n",
    "    if g_arch_double_bottlneck == 1: \n",
    "        g_double_neurons = True\n",
    "        g_bottlneck_neurons= False\n",
    "    elif g_arch_double_bottlneck == 2: \n",
    "        g_double_neurons = False\n",
    "        g_bottlneck_neurons= True\n",
    "    elif g_arch_double_bottlneck == 3: \n",
    "        g_double_neurons = False\n",
    "        g_bottlneck_neurons= False\n",
    "            \n",
    "    if g_b_n == 0:\n",
    "        g_batch_norm=False\n",
    "    else:\n",
    "        g_batch_norm=True\n",
    "    \n",
    "    if g_dr == 0:\n",
    "        g_batch_dropout=False\n",
    "    else: \n",
    "        g_batch_dropout=True\n",
    "        \n",
    "    g_dropout_rate=int_to_dropout_rate[g_b_r]\n",
    "    g_kernel_bias_reg=g_k_b_r\n",
    "    generator_lr=int_to_learning_rate[g_lr]\n",
    "    g_l1_rate=int_to_l1_rate[g_l1_r]\n",
    "    g_l2_rate=int_to_l2_rate[g_l2_r]\n",
    "\n",
    "    #################\n",
    "    # here I privide 8 options for each to not accidentally fail durring hp tuning\n",
    "    encoder_noise_hp_dict = \\\n",
    "    {1: {\n",
    "        1:{'noise_dims':8,'encoder_start_num_neurons':128, 'encoder_n_layers':5},\n",
    "        2:{'noise_dims':16,'encoder_start_num_neurons':128, 'encoder_n_layers':4},\n",
    "        3:{'noise_dims':32,'encoder_start_num_neurons':128, 'encoder_n_layers':3},\n",
    "        4:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        5:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        6:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        7:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2}\n",
    "    },\n",
    "\n",
    "    2: {\n",
    "        1:{'noise_dims':8,'encoder_start_num_neurons':256, 'encoder_n_layers':6},\n",
    "        2:{'noise_dims':16,'encoder_start_num_neurons':256, 'encoder_n_layers':5},\n",
    "        3:{'noise_dims':32,'encoder_start_num_neurons':256, 'encoder_n_layers':4},\n",
    "        4:{'noise_dims':64,'encoder_start_num_neurons':256, 'encoder_n_layers':3},\n",
    "        5:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2},\n",
    "        6:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2},\n",
    "        7:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2}        \n",
    "    }, \n",
    "\n",
    "    3: {\n",
    "        1:{'noise_dims':8, 'encoder_start_num_neurons':512, 'encoder_n_layers':7},\n",
    "        2:{'noise_dims':16, 'encoder_start_num_neurons':512, 'encoder_n_layers':6},\n",
    "        3:{'noise_dims':32, 'encoder_start_num_neurons':512, 'encoder_n_layers':5},\n",
    "        4:{'noise_dims':64, 'encoder_start_num_neurons':512, 'encoder_n_layers':4},\n",
    "        5:{'noise_dims':128, 'encoder_start_num_neurons':512, 'encoder_n_layers':3},\n",
    "        6:{'noise_dims':256, 'encoder_start_num_neurons':512, 'encoder_n_layers':2},\n",
    "        7:{'noise_dims':256, 'encoder_start_num_neurons':512, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':256, 'encoder_start_num_neurons':512, 'encoder_n_layers':2}        \n",
    "    }, \n",
    "\n",
    "    4: {\n",
    "        1:{'noise_dim':8,'encoder_start_num_neurons':1024, 'encoder_n_layers':8},\n",
    "        2:{'noise_dims':16,'encoder_start_num_neurons':1024, 'encoder_n_layers':7},\n",
    "        3:{'noise_dims':32,'encoder_start_num_neurons':1024, 'encoder_n_layers':6},\n",
    "        4:{'noise_dims':64,'encoder_start_num_neurons':1024, 'encoder_n_layers':5},\n",
    "        5:{'noise_dims':128,'encoder_start_num_neurons':1024, 'encoder_n_layers':4},\n",
    "        6:{'noise_dims':256,'encoder_start_num_neurons':1024, 'encoder_n_layers':3},        \n",
    "        7:{'noise_dims':512,'encoder_start_num_neurons':1024, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':512,'encoder_start_num_neurons':1024, 'encoder_n_layers':2}\n",
    "    },\n",
    "\n",
    "    5: {\n",
    "        1:{'noise_dims':8, 'encoder_start_num_neurons':2048, 'encoder_n_layers':9},\n",
    "        2:{'noise_dims':16, 'encoder_start_num_neurons':2048, 'encoder_n_layers':8},\n",
    "        3:{'noise_dims':32, 'encoder_start_num_neurons':2048, 'encoder_n_layers':7},\n",
    "        4:{'noise_dims':64, 'encoder_start_num_neurons':2048, 'encoder_n_layers':6},\n",
    "        5:{'noise_dims':128, 'encoder_start_num_neurons':2048, 'encoder_n_layers':5},\n",
    "        6:{'noise_dims':256, 'encoder_start_num_neurons':2048, 'encoder_n_layers':4},\n",
    "        7:{'noise_dims':512, 'encoder_start_num_neurons':2048, 'encoder_n_layers':3},\n",
    "        8:{'noise_dims':1024, 'encoder_start_num_neurons':2048, 'encoder_n_layers':2}\n",
    "    }}\n",
    "\n",
    "#     # all these variables are very realate so we will select  generator noise dimentions \n",
    "#     # based on combination between g_n_neurons and encoder_n_layers. encoder encoder_start_num_neurons\n",
    "#     # will be the same as g_n_neurons       \n",
    "\n",
    "    if g_n_neurons == 2048:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[5][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 1024:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[4][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 512:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[3][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 256:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[2][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 128:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[1][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    #################\n",
    "    \n",
    "    joint_train=False\n",
    "\n",
    "    # ML Infra.\n",
    "    experiment_type='train'\n",
    "    #model_dir=logdir\n",
    "    num_gpus_per_worker=1 #hops.devices.get_num_gpus()\n",
    "    num_train_steps=50\n",
    "    num_eval_steps=1\n",
    "\n",
    "    num_summary_steps=1\n",
    "    log_step_count_steps=1\n",
    "    save_checkpoints_steps=1\n",
    "        \n",
    "    num_reader_parallel_calls=1\n",
    "    use_dummy_data=False\n",
    "    \n",
    "    #########################################################\n",
    "    data_dir = \"hdfs:///Projects/gan_test/gan_test_Training_Datasets/\"\n",
    "    ben_dataset_dir = pydoop.path.abspath(data_dir + \"train.tfrecord\")\n",
    "    ben_input_files = tf.io.gfile.glob(ben_dataset_dir + \"/part-r-*\")\n",
    "    eval_dataset_dir = pydoop.path.abspath(data_dir + \"eval.tfrecord\") \n",
    "    eval_input_files = tf.io.gfile.glob(eval_dataset_dir + \"/part-r-*\")\n",
    "    \n",
    "    training_dataset = ben_input_files\n",
    "    eval_dataset = eval_input_files\n",
    "    label_name = \"target\"\n",
    "    #########################################################\n",
    "        \n",
    "    hparams = train_gan_enc_experiment_lib.HParams(\n",
    "        \n",
    "      model_name,  \n",
    "\n",
    "      n_epochs,\n",
    "      data_size,\n",
    "      batch_size,\n",
    "      noise_dims,\n",
    "      g_output_dim,\n",
    "      d_output_dim,\n",
    "      feature_dim,\n",
    "      time_steps,\n",
    "      timeseries,\n",
    "\n",
    "      gp_weight,\n",
    "\n",
    "      d_n_neurons,\n",
    "      d_n_layers,\n",
    "      d_activation_fn,\n",
    "      d_double_neurons,\n",
    "      d_bottlneck_neurons,\n",
    "      d_batch_norm,\n",
    "      d_batch_dropout,\n",
    "      d_dropout_rate,\n",
    "      d_kernel_bias_reg,\n",
    "      discriminator_lr,\n",
    "      d_l1_rate,\n",
    "      d_l2_rate,\n",
    "\n",
    "      g_n_neurons,\n",
    "      g_n_layers,\n",
    "      g_activation_fn,\n",
    "      g_double_neurons,\n",
    "      g_bottlneck_neurons,\n",
    "      g_batch_norm,\n",
    "      g_batch_dropout,\n",
    "      g_dropout_rate,\n",
    "      g_kernel_bias_reg,\n",
    "      generator_lr,\n",
    "      g_l1_rate,\n",
    "      g_l2_rate,\n",
    "\n",
    "      encoder_start_num_neurons,\n",
    "      encoder_n_layers,\n",
    "\n",
    "      joint_train,\n",
    "\n",
    "      experiment_type,\n",
    "\n",
    "      num_train_steps,\n",
    "      num_eval_steps,\n",
    "      num_summary_steps,\n",
    "      log_step_count_steps,\n",
    "      save_checkpoints_steps,\n",
    "\n",
    "      training_dataset, \n",
    "      eval_dataset,\n",
    "      label_name,  \n",
    "        \n",
    "      num_reader_parallel_calls,\n",
    "      use_dummy_data,\n",
    "        \n",
    "      0.5,\n",
    "      1   \n",
    "    )\n",
    "    \n",
    "    eval_result = train_gan_enc_experiment_lib.train(hparams)\n",
    "    return eval_result\n",
    "#     reporter.broadcast(metric=eval_result[\"loss\"])\n",
    "#     return eval_result[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hops import experiment\n",
    "#from hops.experiment import Direction\n",
    "#def hyperparam_search():\n",
    "#    search_dict = {\n",
    "#        \n",
    "#                'gp_w': [1,3],   # gp_weight\n",
    "#                'd_n': [2,4],    # d_n_neurons\n",
    "#                'd_n_l': [1,4],  # d_n_layers\n",
    "#                'd_a_f': [1,4],  # d_activation_fn\n",
    "#                'd_a_d_b': [1,3],  # d_double_neurons or d_bottlneck_neurons\n",
    "#                'd_b_n': [0,1],  # d_batch_norm\n",
    "#                'd_dr': [0,1],   # d_batch_dropout\n",
    "#                'd_b_r': [1,4],  # d_dropout_rate\n",
    "#                'd_lr': [1,4],   # d_learning_rate\n",
    "#                'd_k_b_r': [1,4], # d_kernel_bias_reg\n",
    "#                'd_l1_r': [1,4], # int_to_l1_rate\n",
    "#                'd_l2_r': [1,4], # int_to_l2_rate\n",
    "#\n",
    "#                'g_n': [5,6],    # g_n_neurons\n",
    "#                'g_n_l': [1,4],  # g_n_layers\n",
    "#                'g_a_f': [1,4],  # g_activation_fn\n",
    "#                'g_a_d_b': [1,3],  # g_double_neurons or g_bottlneck_neurons\n",
    "#                'g_b_n': [0,1],  # g_batch_norm\n",
    "#                'g_dr': [0,1],   # g_batch_dropout\n",
    "#                'g_b_r': [1,4],  # g_dropout_rate\n",
    "#                'g_k_b_r': [1,4], # g_kernel_bias_reg\n",
    "#                'g_lr': [1,4],    # generator_lr\n",
    "#                'g_l1_r': [1,4], #g_l1_rate\n",
    "#                'g_l2_r': [1,4], #g_l2_rate\n",
    "#\n",
    "#                'en_hp': [1,8]  # encoder_n_layers        \n",
    "#\n",
    "#    }\n",
    "#    \n",
    "#    log_dir, best_params = experiment.differential_evolution(\n",
    "#    gan_main, \n",
    "#    search_dict, \n",
    "#    name='gan_enc_search', \n",
    "#    description='GAN anomaly encoder search',\n",
    "#    local_logdir=True, \n",
    "#    population=8,\n",
    "#    generations = 10,\n",
    "#    direction=Direction.MIN, \n",
    "#    optimization_key='loss'    \n",
    "#    )\n",
    "#    return log_dir, best_params\n",
    "#\n",
    "#log_dir, best_params = hyperparam_search()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "# experiment.collective_all_reduce(main)\n",
    "experiment.launch(gan_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = experiment.lagom(map_fun=main, \n",
    "#                            searchspace=sp, \n",
    "#                            optimizer='randomsearch', \n",
    "#                            direction='min',\n",
    "#                            num_trials=15, \n",
    "#                            name='gan_enc_search', \n",
    "#                            hb_interval=5, \n",
    "#                            es_interval=5,\n",
    "#                            es_min=5\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"ganomaly\"\n",
    "EVALUATION_METRIC=\"encoder_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ganomaly\n",
      "Model version: 1\n",
      "{'encoder_loss': '949.6477661132812'}"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a serving for model ganomaly ...\n",
      "Serving for model ganomaly successfully created"
     ]
    }
   ],
   "source": [
    "# Create serving\n",
    "model_path=\"/Models/\" + best_model['name']\n",
    "response = serving.create_or_update(model_path, MODEL_NAME, serving_type=\"TENSORFLOW\", \n",
    "                                 model_version=best_model['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afmnist\n",
      "ganomaly"
     ]
    }
   ],
   "source": [
    "# List all available servings in the project\n",
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Stopped'"
     ]
    }
   ],
   "source": [
    "# Get serving status\n",
    "serving.get_status(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: ganomaly...\n",
      "Serving with name: ganomaly successfully started"
     ]
    }
   ],
   "source": [
    "if serving.get_status(MODEL_NAME) == 'Stopped':\n",
    "    serving.start(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(MODEL_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [1724.10559]}\n",
      "{'predictions': [347.750244]}\n",
      "{'predictions': [744.655]}\n",
      "{'predictions': [2042.27856]}\n",
      "{'predictions': [557.614258]}\n",
      "{'predictions': [315.85498]}\n",
      "{'predictions': [1655.20459]}\n",
      "{'predictions': [316.268921]}\n",
      "{'predictions': [1744.2124]}\n",
      "{'predictions': [1149.82349]}\n",
      "{'predictions': [1569.00781]}\n",
      "{'predictions': [468.74588]}\n",
      "{'predictions': [960.264221]}\n",
      "{'predictions': [1766.28601]}\n",
      "{'predictions': [985.822876]}\n",
      "{'predictions': [1204.39709]}\n",
      "{'predictions': [399.651917]}\n",
      "{'predictions': [440.584076]}\n",
      "{'predictions': [1225.974]}\n",
      "{'predictions': [208.740234]}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(20):\n",
    "    data = {\n",
    "                'serving_default': 'real_input',\n",
    "                \"instances\": [np.random.rand(365).astype(np.float32).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(MODEL_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.random.rand(365).astype(np.float32).tolist(),np.random.rand(365).astype(np.float32).tolist(),np.random.rand(365).astype(np.float32).tolist(), np.random.rand(365).astype(np.float32).tolist()]\n",
    "rdd = sc.parallelize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead"
     ]
    }
   ],
   "source": [
    "people = rdd.map(lambda x: { 'serving_default': 'real_input', \"instances\": [x] }).map(lambda x:  serving.make_inference_request(MODEL_NAME, x)).toDF()\n",
    "                 \n",
    "#rdd.map(lambda x: np.append(arr, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| predictions|\n",
      "+------------+\n",
      "|[686.622559]|\n",
      "|[1406.82849]|\n",
      "|[681.662476]|\n",
      "|[520.253235]|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
