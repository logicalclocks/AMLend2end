{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>34</td><td>application_1606998977104_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1606998977104_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-88.us-west-2.compute.internal:8042/node/containerlogs/container_e04_1606998977104_0001_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fe836193e90>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_fg = fs.get_feature_group(\"edge_features\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"hdfs:///Projects/{}/Resources/embeddings_features_80_5_128.csv\".format(hdfs.project_name(),inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.columns\n",
    "feature_names = [\"_\" + s + \"c\"  for s in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(*feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__c0c', '_0c', '_1c', '_2c', '_3c', '_4c', '_5c', '_6c', '_7c', '_8c', '_9c', '_10c', '_11c', '_12c', '_13c', '_14c', '_15c', '_16c', '_17c', '_18c', '_19c', '_20c', '_21c', '_22c', '_23c', '_24c', '_25c', '_26c', '_27c', '_28c', '_29c', '_30c', '_31c', '_32c', '_33c', '_34c', '_35c', '_36c', '_37c', '_38c', '_39c', '_40c', '_41c', '_42c', '_43c', '_44c', '_45c', '_46c', '_47c', '_48c', '_49c', '_50c', '_51c', '_52c', '_53c', '_54c', '_55c', '_56c', '_57c', '_58c', '_59c', '_60c', '_61c', '_62c', '_63c', '_64c', '_65c', '_66c', '_67c', '_68c', '_69c', '_70c', '_71c', '_72c', '_73c', '_74c', '_75c', '_76c', '_77c', '_78c', '_79c', '_80c', '_81c', '_82c', '_83c', '_84c', '_85c', '_86c', '_87c', '_88c', '_89c', '_90c', '_91c', '_92c', '_93c', '_94c', '_95c', '_96c', '_97c', '_98c', '_99c', '_100c', '_101c', '_102c', '_103c', '_104c', '_105c', '_106c', '_107c', '_108c', '_109c', '_110c', '_111c', '_112c', '_113c', '_114c', '_115c', '_116c', '_117c', '_118c', '_119c', '_120c', '_121c', '_122c', '_123c', '_124c', '_125c', '_126c', '_127c']"
     ]
    }
   ],
   "source": [
    "feature_names = df.columns\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[0]= 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = df.toDF(*feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_fg = node_embeddings.join(labels, ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, coalesce      \n",
    "\n",
    "df.select(concat(\n",
    "    coalesce(col(\"tokens\"), array()),\n",
    "    coalesce(col(\"tokens_bigrams\"), array())\n",
    ")).show(truncate = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
