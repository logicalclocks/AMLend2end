{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>25</td><td>application_1607949680860_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607949680860_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworksdavit-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e02_1607949680860_0027_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f5bf047efd0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import hsfs\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashnode(x):\n",
    "    return hashlib.sha1(x.encode(\"UTF-8\")).hexdigest()[:8]\n",
    "\n",
    "def extract_node_type(x):\n",
    "    if (x.startswith(\"C\")):\n",
    "        node_type = 0\n",
    "    elif (x.startswith(\"B\")):\n",
    "        node_type = 1\n",
    "    elif (x.startswith(\"M\")):\n",
    "        node_type = 2\n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def extract_fraudster(x):\n",
    "    if (x.startswith(\"CF\")):\n",
    "        fraudster = 1\n",
    "    else:\n",
    "        fraudster = 0\n",
    "    return fraudster\n",
    "\n",
    "def action_2_code(x):\n",
    "    if (x == \"CASH_IN\"):\n",
    "        node_type = 0\n",
    "    elif (x == \"CASH_OUT\"):\n",
    "        node_type = 1\n",
    "    elif (x == \"DEBIT\"):\n",
    "        node_type = 2\n",
    "    elif (x == \"PAYMENT\"):\n",
    "        node_type = 3\n",
    "    elif (x == \"TRANSFER\"):\n",
    "        node_type = 4\n",
    "    elif (x == \"DEPOSIT\"):\n",
    "        node_type = 4        \n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def gender_2_code(x):\n",
    "    if x == 'W':\n",
    "        gender =  0\n",
    "    elif x == 'M':\n",
    "        gender = 1\n",
    "    else: \n",
    "        gender = 99\n",
    "    return gender\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    return dt_obj.timestamp()\n",
    "\n",
    "hashnode_udf = F.udf(hashnode)\n",
    "extract_fraudster_udf = F.udf(extract_fraudster)\n",
    "node_type_udf = F.udf(extract_node_type)\n",
    "action_2_code_udf = F.udf(action_2_code)\n",
    "gender_2_code_udf = F.udf(gender_2_code)\n",
    "timestamp_2_time_udf = F.udf(timestamp_2_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to Hopsworks feature store (hsfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load accounts datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/accounts.csv\".format(hdfs.project_name()))\n",
    "\n",
    "accounts_df = accounts_df.drop('first_name')\\\n",
    "                        .drop('last_name')\\\n",
    "                        .drop('street_addr')\\\n",
    "                        .drop('city')\\\n",
    "                        .drop('state')\\\n",
    "                        .drop('zip')\\\n",
    "                        .drop('gender')\\\n",
    "                        .drop('birth_date')\\\n",
    "                        .drop('ssn')\\\n",
    "                        .drop('lon')\\\n",
    "                        .drop('lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.withColumn('prior_sar',F.when(F.col('prior_sar_count') == 'true', 1).otherwise(0))\\\n",
    "                         .drop(\"prior_sar_count\",\"acct_rptng_crncy\",\"type\",\"acct_stat\",\"open_dt\",\"bank_id\",\"country\",\"close_dt\",\"dsply_nm\",\"branch_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create accounts feature group metadata and save it in to hsfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7f5bffd5b990>"
     ]
    }
   ],
   "source": [
    "accounts_fg = fs.create_feature_group(name=\"account_features\",\n",
    "                                      version=1,\n",
    "                                      primary_key=[\"acct_id\"],\n",
    "                                      description=\"node features\",\n",
    "                                      time_travel_format=None,\n",
    "                                      statistics_config=False)\n",
    "accounts_fg.save(accounts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/transactions.csv\".format(hdfs.project_name()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------+------+--------+-------+-------+--------+\n",
      "|source|target|tran_timestamp|is_sar|alert_id|tran_id|tx_type|base_amt|\n",
      "+------+------+--------------+------+--------+-------+-------+--------+\n",
      "|  1767|  3259|   1.4832288E9|     0|      -1|      1|      4| 9405.71|\n",
      "|  7363|  5141|   1.4832288E9|     0|      -1|      2|      4| 6884.54|\n",
      "|  7585|  9532|   1.4832288E9|     0|      -1|      3|      4|  7968.4|\n",
      "|  1750|  8792|   1.4832288E9|     0|      -1|      4|      4| 9042.67|\n",
      "|  9060|  4670|   1.4832288E9|     0|      -1|      5|      4| 4692.79|\n",
      "|  8752|  3861|   1.4832288E9|     0|      -1|      6|      4| 4089.65|\n",
      "|  9645|  3805|   1.4832288E9|     0|      -1|      7|      4| 3055.04|\n",
      "|  7150|  9280|   1.4832288E9|     0|      -1|      8|      4| 6473.45|\n",
      "|  1685|  6369|   1.4832288E9|     0|      -1|      9|      4| 2583.42|\n",
      "|  7846|  8255|   1.4832288E9|     0|      -1|     10|      4| 6753.04|\n",
      "|   878|  5957|   1.4832288E9|     0|      -1|     11|      4| 1743.57|\n",
      "|   797|  4091|   1.4832288E9|     0|      -1|     12|      4| 8591.59|\n",
      "|  8472|  5274|   1.4832288E9|     0|      -1|     13|      4| 7006.77|\n",
      "|  5584|  3296|   1.4832288E9|     0|      -1|     14|      4| 5095.46|\n",
      "|   616|  7618|   1.4832288E9|     0|      -1|     15|      4|  2537.2|\n",
      "|  7312|  7617|   1.4832288E9|     0|      -1|     16|      4| 3120.27|\n",
      "|  9886|  9987|   1.4832288E9|     0|      -1|     17|      4| 5308.95|\n",
      "|  1132|  3603|   1.4832288E9|     0|      -1|     18|      4| 6788.85|\n",
      "|   266|  3844|   1.4832288E9|     0|      -1|     19|      4| 5708.25|\n",
      "|  2184|  2811|   1.4832288E9|     0|      -1|     20|      4| 5020.95|\n",
      "+------+------+--------------+------+--------+-------+-------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('is_sar',F.when(F.col('is_sar') == 'true', 1).otherwise(0))\\\n",
    "                                 .withColumn('tx_type', action_2_code_udf(F.col('tx_type')))\\\n",
    "                                 .withColumn('tran_timestamp', timestamp_2_time_udf(F.col('tran_timestamp')).cast(FloatType()))\\\n",
    "                                 .withColumnRenamed(\"orig_acct\",\"source\")\\\n",
    "                                 .withColumnRenamed(\"bene_acct\",\"target\")\\\n",
    "                                 .select(\"source\",\"target\",\"tran_timestamp\",\"is_sar\",\"alert_id\",\"tran_id\",\"tx_type\",\"base_amt\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transactions feature group metadata and save it in to hsfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7f5bffd21ed0>"
     ]
    }
   ],
   "source": [
    "transactions_fg = fs.create_feature_group(name=\"transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "                                       description=\"edge features\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "transactions_fg.save(transactions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
