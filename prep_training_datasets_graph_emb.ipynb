{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f7149742f90>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "from graphframes import *\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import FloatType\n",
    "import hsfs\n",
    "from hops import hdfs\n",
    "import os\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashnode(x):\n",
    "    return hashlib.sha1(x.encode(\"UTF-8\")).hexdigest()[:8]\n",
    "\n",
    "hashnode_udf = func.udf(hashnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_fg = fs.get_feature_group('transactions_fg', 1)\n",
    "node_fg = fs.get_feature_group('account_features', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o446.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/feature_group.py\", line 223, in show\n",
      "    return self.select_all().show(n, online)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/query.py\", line 92, in show\n",
      "    sql_query, self._feature_store_name, n, online_conn\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/engine/spark.py\", line 75, in show\n",
      "    return self.sql(sql_query, feature_store, online_conn, \"default\").show(n)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 378, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o446.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "node_fg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o490.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/feature_group.py\", line 223, in show\n",
      "    return self.select_all().show(n, online)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/query.py\", line 92, in show\n",
      "    sql_query, self._feature_store_name, n, online_conn\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/engine/spark.py\", line 75, in show\n",
      "    return self.sql(sql_query, feature_store, online_conn, \"default\").show(n)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 378, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o490.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edge_fg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_sar_edge_df = edge_fg.read().filter(func.col('alert_id') != -1)\n",
    "only_normal_edge_df =  edge_fg.read().filter(func.col('alert_id') == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o541.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 378, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o541.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "only_normal_edge_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o536.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 378, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o536.showString.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:53)\n",
      "org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:67)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:88)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340)\n",
      "org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:63)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128)\n",
      "org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486)\n",
      "\tat org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.buildReaderWithPartitionValues(OrcFileFormat.scala:171)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:309)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:305)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:327)\n",
      "\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "only_sar_edge_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_sar_edge_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732"
     ]
    }
   ],
   "source": [
    "sar_sources = only_sar_edge_df.select(\"source\")\n",
    "sar_targets = only_sar_edge_df.select(\"target\")\n",
    "sar_nodes = sar_sources.union(sar_targets).toDF(\"id\").dropDuplicates()\n",
    "sar_nodes.count()\n",
    "\n",
    "sar_edges = only_sar_edge_df.select(\"source\", \"target\").toDF(\"src\", \"dst\")\n",
    "sar_edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets construct the graph\n",
    "g_sar = GraphFrame(sar_nodes,sar_edges)\n",
    "sc.setCheckpointDir(\"hdfs:///Projects/{}/Logs/sc\".format(hdfs.project_name()))\n",
    "cc_sar = g_sar.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|  id|component|\n",
      "+----+---------+\n",
      "|5300|     2773|\n",
      "| 463|      124|\n",
      "|3997|     2773|\n",
      "|8086|     2599|\n",
      "|7833|     3671|\n",
      "|1127|     1127|\n",
      "| 540|      540|\n",
      "|6393|     3396|\n",
      "|5614|      825|\n",
      "|1522|     1054|\n",
      "|3488|     2599|\n",
      "|2393|      397|\n",
      "|9162|     2889|\n",
      "|7387|      643|\n",
      "|1265|     1009|\n",
      "|4042|     1590|\n",
      "|5223|     1013|\n",
      "|4364|     1096|\n",
      "|3425|     1054|\n",
      "|5157|       61|\n",
      "+----+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "cc_sar.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "+-----+"
     ]
    }
   ],
   "source": [
    "cc_sar.groupBy('component').count().select('count').dropDuplicates().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_sar = g_sar.stronglyConnectedComponents(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|  id|component|\n",
      "+----+---------+\n",
      "|7800|     7800|\n",
      "|8600|     8600|\n",
      "|4000|     4000|\n",
      "|5201|     1109|\n",
      "|1801|     1801|\n",
      "| 601|      601|\n",
      "|1201|     1201|\n",
      "|4601|     4601|\n",
      "|9601|     9601|\n",
      "|3402|     3402|\n",
      "|4802|     4802|\n",
      "|7802|     1945|\n",
      "|6403|     1630|\n",
      "|1003|     1003|\n",
      "|2003|     1201|\n",
      "|5403|      295|\n",
      "|9403|     9403|\n",
      "|7604|     1201|\n",
      "|5204|     1257|\n",
      "|8004|     8004|\n",
      "+----+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "scc_sar.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    1|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "+-----+"
     ]
    }
   ],
   "source": [
    "scc_sar.groupBy('component').count().select('count').dropDuplicates().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471"
     ]
    }
   ],
   "source": [
    "scc_sar.groupBy('component').count().where(func.col('count')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40"
     ]
    }
   ],
   "source": [
    "scc_sar.groupBy('component').count().where(func.col('count')>1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_comp_count = scc_sar.groupBy('component').count().where(func.col('count')>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_sar = scc_sar.join(scc_comp_count,['component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|component|  id|count|\n",
      "+---------+----+-----+\n",
      "|     1152|3595|   10|\n",
      "|     1152|1152|   10|\n",
      "|     1152|8535|   10|\n",
      "|     1152|4324|   10|\n",
      "|     1152|2321|   10|\n",
      "|     1152|8117|   10|\n",
      "|     1152|3304|   10|\n",
      "|     1152|8654|   10|\n",
      "|     1152|8049|   10|\n",
      "|     1152|7824|   10|\n",
      "|      399| 399|    6|\n",
      "|      399|8965|    6|\n",
      "|      399|9960|    6|\n",
      "|      399|7146|    6|\n",
      "|      399|2501|    6|\n",
      "|      399|5621|    6|\n",
      "|     3671|4969|    6|\n",
      "|     3671|8546|    6|\n",
      "|     3671|3671|    6|\n",
      "|     3671|4867|    6|\n",
      "+---------+----+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "scc_sar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_sar =  scc_sar.drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_cc_grouped = cc_sar.join(\n",
    "    only_sar_edge_df,\n",
    "    [(only_sar_edge_df.source==cc_sar.id)|(only_sar_edge_df.target==cc_sar.id)],\n",
    "    how=\"left\"\n",
    ").dropDuplicates(subset=['tran_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_scc_grouped = scc_sar.join(\n",
    "    only_sar_edge_df,\n",
    "    [(only_sar_edge_df.source==scc_sar.id)|(only_sar_edge_df.target==scc_sar.id)],\n",
    "    how=\"left\"\n",
    ").dropDuplicates(subset=['tran_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732"
     ]
    }
   ],
   "source": [
    "sar_cc_grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321"
     ]
    }
   ],
   "source": [
    "sar_scc_grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732"
     ]
    }
   ],
   "source": [
    "only_sar_edge_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+\n",
      "|component|  id|tx_type|is_sar|tran_id|tran_timestamp|base_amt|target|alert_id|source|\n",
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+\n",
      "|     1009|1425|      4|     1| 100274|   1.4892768E9|  102.86|  1425|      80|  3661|\n",
      "|     1801|9700|      4|     1| 284874|   1.5004224E9|  103.47|  3984|      96|  9700|\n",
      "|      554|8091|      4|     1| 522545|   1.5147648E9|  106.94|  8091|      91|   554|\n",
      "|      825|4571|      4|     1| 706588|    1.525824E9|   63.66|  7776|      84|  4571|\n",
      "|     1369|1598|      4|     1| 472208|   1.5117408E9|  114.67|  8581|      70|  1598|\n",
      "|     1257|5261|      4|     1|  57885|   1.4866848E9|   73.41|  6837|      95|  5261|\n",
      "|      907|7851|      4|     1| 509573|   1.5139872E9|   47.28|  3329|      90|  7851|\n",
      "|      397|3252|      4|     1| 786028|   1.5306624E9|   79.42|  6415|      74|  3252|\n",
      "|      295|6810|      4|     1| 202624|   1.4954112E9|   57.47|  9785|      99|  6810|\n",
      "|     3669|5541|      4|     1|  93978|   1.4888448E9|  173.69|  5541|      85|  3669|\n",
      "|     2047|2542|      4|     1| 179206|   1.4940288E9|  197.69|  2542|      81|  2861|\n",
      "|     2047|7703|      4|     1| 186510|   1.4944608E9|   129.7|  7703|      81|  6767|\n",
      "|     1369|6504|      4|     1| 474798|   1.5118272E9|   92.88|  1926|      70|  6504|\n",
      "|     1945|7802|      4|     1| 769037|   1.5296256E9|  143.74|  4743|      98|  7802|\n",
      "|     1257|5204|      4|     1|  35905|   1.4853888E9|  153.47|  5204|      95|  3441|\n",
      "|      601|6315|      4|     1| 405475|     1.50768E9|  179.36|  6315|      63|  9965|\n",
      "|      374|1826|      4|     1|  72841|   1.4875488E9|  174.29|  1826|      72|   374|\n",
      "|      825|5440|      4|     1| 701190|   1.5255648E9|   78.59|  5440|      84|  5614|\n",
      "|     1945|1945|      4|     1| 774140|   1.5299712E9|  116.43|  1945|      98|  7176|\n",
      "|      601|9965|      4|     1| 407998|   1.5078528E9|  130.76|  9965|      63|  9223|\n",
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sar_scc_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_sar_edge_df = sar_scc_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+\n",
      "|component|  id|tx_type|is_sar|tran_id|tran_timestamp|base_amt|target|alert_id|source|\n",
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+\n",
      "|     1009|3661|      4|     1| 100274|   1.4892768E9|  102.86|  1425|      80|  3661|\n",
      "|     1801|3984|      4|     1| 284874|   1.5004224E9|  103.47|  3984|      96|  9700|\n",
      "|      554|8091|      4|     1| 522545|   1.5147648E9|  106.94|  8091|      91|   554|\n",
      "|      825|4571|      4|     1| 706588|    1.525824E9|   63.66|  7776|      84|  4571|\n",
      "|     1369|8581|      4|     1| 472208|   1.5117408E9|  114.67|  8581|      70|  1598|\n",
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "only_sar_edge_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028964"
     ]
    }
   ],
   "source": [
    "only_normal_edge_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|component|window_start| window_end|\n",
      "+---------+------------+-----------+\n",
      "|     1152|  1.516752E9|1.5170976E9|\n",
      "|      399| 1.5412032E9|1.5428448E9|\n",
      "|     3671| 1.5287616E9| 1.530144E9|\n",
      "|     3751| 1.4901408E9|1.4911776E9|\n",
      "|     1945| 1.5290208E9|1.5299712E9|\n",
      "+---------+------------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "only_sar_edge_df_grouped = only_sar_edge_df.groupBy('component').agg(func.min(\"tran_timestamp\"),func.max(\"tran_timestamp\")).toDF(\"component\", \"window_start\", \"window_end\")\n",
    "only_sar_edge_df_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_sar_edges_df_windows = only_sar_edge_df.join(only_sar_edge_df_grouped,[\"component\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+------------+-----------+\n",
      "|component|  id|tx_type|is_sar|tran_id|tran_timestamp|base_amt|target|alert_id|source|window_start| window_end|\n",
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+------------+-----------+\n",
      "|     1152|8654|      4|     1| 561549|   1.5170976E9|    43.6|  8654|      65|  8535|  1.516752E9|1.5170976E9|\n",
      "|     1152|3304|      4|     1| 559541|   1.5170112E9|   73.84|  3304|      65|  8049|  1.516752E9|1.5170976E9|\n",
      "|     1152|4324|      4|     1| 561547|   1.5170976E9|   53.83|  2321|      65|  4324|  1.516752E9|1.5170976E9|\n",
      "|     1152|1152|      4|     1| 556095|    1.516752E9|  101.29|  8535|      65|  1152|  1.516752E9|1.5170976E9|\n",
      "|     1152|8049|      4|     1| 561548|   1.5170976E9|   48.44|  8049|      65|  2321|  1.516752E9|1.5170976E9|\n",
      "|     1152|3304|      4|     1| 559542|   1.5170112E9|   66.45|  8117|      65|  3304|  1.516752E9|1.5170976E9|\n",
      "|     1152|8117|      4|     1| 559198|   1.5169248E9|   82.04|  7824|      65|  8117|  1.516752E9|1.5170976E9|\n",
      "|     1152|3595|      4|     1| 556094|    1.516752E9|  112.55|  3595|      65|  8654|  1.516752E9|1.5170976E9|\n",
      "|     1152|3595|      4|     1| 561546|   1.5170976E9|   59.81|  4324|      65|  3595|  1.516752E9|1.5170976E9|\n",
      "|     1152|1152|      4|     1| 557719|   1.5168384E9|   91.16|  1152|      65|  7824|  1.516752E9|1.5170976E9|\n",
      "|      399| 399|      4|     1| 961039|   1.5412896E9|  103.84|  5621|      78|   399| 1.5412032E9|1.5428448E9|\n",
      "|      399|9960|      4|     1| 988030|   1.5428448E9|   68.13|  9960|      78|  2501| 1.5412032E9|1.5428448E9|\n",
      "|      399|8965|      4|     1| 981236|   1.5424992E9|   84.11|  8965|      78|  5621| 1.5412032E9|1.5428448E9|\n",
      "|      399|7146|      4|     1| 977325|     1.54224E9|   93.45|   399|      78|  7146| 1.5412032E9|1.5428448E9|\n",
      "|      399|7146|      4|     1| 986670|   1.5427584E9|    75.7|  7146|      78|  9960| 1.5412032E9|1.5428448E9|\n",
      "|      399|2501|      4|     1| 960047|   1.5412032E9|  115.37|  2501|      78|  8965| 1.5412032E9|1.5428448E9|\n",
      "|     3671|3671|      4|     1| 754052|   1.5287616E9|   132.8|  3671|      92|  4867| 1.5287616E9| 1.530144E9|\n",
      "|     3671|4969|      4|     1| 777776|    1.530144E9|   78.41|  4969|      92|  3671| 1.5287616E9| 1.530144E9|\n",
      "|     3671|4867|      4|     1| 757136|   1.5289344E9|  107.56|  4867|      92|  8546| 1.5287616E9| 1.530144E9|\n",
      "|     3671|6241|      4|     1| 758500|   1.5290208E9|   96.81|  8546|      92|  6241| 1.5287616E9| 1.530144E9|\n",
      "+---------+----+-------+------+-------+--------------+--------+------+--------+------+------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "only_sar_edges_df_windows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321"
     ]
    }
   ],
   "source": [
    "only_sar_edges_df_windows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_normal_edges_df_windows = only_sar_edge_df_grouped.select(\"window_start\", \"window_end\").join(\n",
    "    only_normal_edge_df,\n",
    "    [(only_normal_edge_df.tran_timestamp>=only_sar_edge_df_grouped.window_start)&(only_normal_edge_df.tran_timestamp<=only_sar_edge_df_grouped.window_end)],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------+------+-------+--------------+--------+------+--------+------+\n",
      "|window_start| window_end|tx_type|is_sar|tran_id|tran_timestamp|base_amt|target|alert_id|source|\n",
      "+------------+-----------+-------+------+-------+--------------+--------+------+--------+------+\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555045|    1.516752E9|  503.89|  9947|      -1|  6249|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555046|    1.516752E9|  564.26|  3095|      -1|  4557|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555047|    1.516752E9| 7773.05|  3003|      -1|  4967|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555048|    1.516752E9|  4490.0|  9866|      -1|  7090|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555049|    1.516752E9| 6136.03|  1343|      -1|  4471|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555050|    1.516752E9| 9344.63|  3076|      -1|  6975|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555051|    1.516752E9| 1527.03|  1969|      -1|  6923|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555052|    1.516752E9| 4742.97|   349|      -1|  4380|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555053|    1.516752E9| 4214.32|  8908|      -1|  6574|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555054|    1.516752E9| 7665.71|  8725|      -1|  6495|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555055|    1.516752E9| 4860.73|   689|      -1|  5156|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555056|    1.516752E9|  7818.5|  6420|      -1|  7693|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555057|    1.516752E9| 5913.22|  4901|      -1|  9776|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555058|    1.516752E9| 2377.77|  9678|      -1|  6412|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555059|    1.516752E9|  8598.2|  9994|      -1|  7412|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555060|    1.516752E9| 6902.87|  9127|      -1|  9421|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555061|    1.516752E9| 7507.27|  7695|      -1|  8992|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555062|    1.516752E9| 3398.08|  7769|      -1|   137|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555063|    1.516752E9| 6324.95|  3726|      -1|  3810|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555064|    1.516752E9| 5511.01|  6878|      -1|  4388|\n",
      "+------------+-----------+-------+------+-------+--------------+--------+------+--------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "only_normal_edges_df_windows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677241"
     ]
    }
   ],
   "source": [
    "only_normal_edges_df_windows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------+------+-------+--------------+--------+------+--------+------+--------+--------+\n",
      "|window_start| window_end|tx_type|is_sar|tran_id|tran_timestamp|base_amt|destId|alert_id|origId|  target|  source|\n",
      "+------------+-----------+-------+------+-------+--------------+--------+------+--------+------+--------+--------+\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555045|    1.516752E9|  503.89|  9947|      -1|  6249|b786ed64|bbc43466|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555046|    1.516752E9|  564.26|  3095|      -1|  4557|b0b7a393|af529315|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555047|    1.516752E9| 7773.05|  3003|      -1|  4967|4156c046|e033872d|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555048|    1.516752E9|  4490.0|  9866|      -1|  7090|05ac06b4|3eac3624|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555049|    1.516752E9| 6136.03|  1343|      -1|  4471|e1901da0|cb468f43|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555050|    1.516752E9| 9344.63|  3076|      -1|  6975|40ad9709|fb953bd3|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555051|    1.516752E9| 1527.03|  1969|      -1|  6923|56a90cea|156546e9|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555052|    1.516752E9| 4742.97|   349|      -1|  4380|defc0df5|db210922|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555053|    1.516752E9| 4214.32|  8908|      -1|  6574|d30039ac|7a745843|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555054|    1.516752E9| 7665.71|  8725|      -1|  6495|05facc21|5f1c26e4|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555055|    1.516752E9| 4860.73|   689|      -1|  5156|9a5e5b36|d1c67237|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555056|    1.516752E9|  7818.5|  6420|      -1|  7693|1367dad7|25f2f82e|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555057|    1.516752E9| 5913.22|  4901|      -1|  9776|62865842|4636f4a5|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555058|    1.516752E9| 2377.77|  9678|      -1|  6412|8186b34d|61aa83b7|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555059|    1.516752E9|  8598.2|  9994|      -1|  7412|6dfa674f|b1a62690|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555060|    1.516752E9| 6902.87|  9127|      -1|  9421|e98dd5f5|b68a6a9b|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555061|    1.516752E9| 7507.27|  7695|      -1|  8992|581c619e|480e7725|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555062|    1.516752E9| 3398.08|  7769|      -1|   137|636c6b62|7e697905|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555063|    1.516752E9| 6324.95|  3726|      -1|  3810|6e2cee21|c0544242|\n",
      "|  1.516752E9|1.5170976E9|      4|     0| 555064|    1.516752E9| 5511.01|  6878|      -1|  4388|f8a13b02|2edf1f54|\n",
      "+------------+-----------+-------+------+-------+--------------+--------+------+--------+------+--------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "only_normal_edges_df_windows = only_normal_edges_df_windows.withColumnRenamed(\"source\", \"origId\")\\\n",
    "                                                           .withColumnRenamed(\"target\", \"destId\")  \n",
    "only_normal_edges_df_windows = only_normal_edges_df_windows.withColumn('target',hashnode_udf(func.concat(func.col('destId'),func.lit('_'),func.col('window_start'),func.lit('_'),func.col('window_end'))))\\\n",
    "                                                           .withColumn('source',hashnode_udf(func.concat(func.col('origId'),func.lit('_'),func.col('window_start'),func.lit('_'),func.col('window_end'))))            \n",
    "only_normal_edges_df_windows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sources = only_normal_edges_df_windows.select(\"source\")\n",
    "normal_targets = only_normal_edges_df_windows.select(\"target\")\n",
    "normal_nodes = normal_sources.union(normal_targets).toDF(\"id\").dropDuplicates()\n",
    "normal_edges = only_normal_edges_df_windows.select(\"source\", \"target\").toDF(\"src\", \"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets construct the graph\n",
    "g_normal = GraphFrame(normal_nodes,normal_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o418.run.\n",
      ": java.lang.IllegalStateException: SparkContext has been shutdown\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n",
      "\tat org.apache.spark.graphx.GraphOps.numVertices$lzycompute(GraphOps.scala:41)\n",
      "\tat org.apache.spark.graphx.GraphOps.numVertices(GraphOps.scala:41)\n",
      "\tat org.apache.spark.graphx.lib.StronglyConnectedComponents$.run(StronglyConnectedComponents.scala:50)\n",
      "\tat org.graphframes.lib.StronglyConnectedComponents$.org$graphframes$lib$StronglyConnectedComponents$$run(StronglyConnectedComponents.scala:51)\n",
      "\tat org.graphframes.lib.StronglyConnectedComponents.run(StronglyConnectedComponents.scala:43)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/graphframes/graphframe.py\", line 418, in stronglyConnectedComponents\n",
      "    jdf = self._jvm_graph.stronglyConnectedComponents().maxIter(maxIter).run()\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o418.run.\n",
      ": java.lang.IllegalStateException: SparkContext has been shutdown\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2053)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n",
      "\tat org.apache.spark.graphx.GraphOps.numVertices$lzycompute(GraphOps.scala:41)\n",
      "\tat org.apache.spark.graphx.GraphOps.numVertices(GraphOps.scala:41)\n",
      "\tat org.apache.spark.graphx.lib.StronglyConnectedComponents$.run(StronglyConnectedComponents.scala:50)\n",
      "\tat org.graphframes.lib.StronglyConnectedComponents$.org$graphframes$lib$StronglyConnectedComponents$$run(StronglyConnectedComponents.scala:51)\n",
      "\tat org.graphframes.lib.StronglyConnectedComponents.run(StronglyConnectedComponents.scala:43)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.setCheckpointDir(\"hdfs:///Projects/{}/Logs/sc\".format(hdfs.project_name()))\n",
    "#cc_normal = g_normal.connectedComponents()\n",
    "scc_normal = g_normal.stronglyConnectedComponents(20).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'scc_normal' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'scc_normal' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scc_norm_comp_count = scc_normal.groupBy('component').count().where(func.col('count')>1)\n",
    "scc_normal = scc_normal.join(scc_norm_comp_count,['component'])\n",
    "scc_normal =  scc_normal.drop('count')\n",
    "normal_scc_grouped = normal_normal.join(\n",
    "    only_normal_edge_df,\n",
    "    [(only_normal_edge_df.source==scc_normal.id)|(only_normal_edge_df.target==scc_normal.id)],\n",
    "    how=\"left\"\n",
    ").dropDuplicates(subset=['tran_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'normal_scc_grouped' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'normal_scc_grouped' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normal_scc_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
