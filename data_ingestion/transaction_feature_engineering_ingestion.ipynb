{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>79</td><td>application_1609265553881_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworksdavit-master.internal.cloudapp.net:8088/proxy/application_1609265553881_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworksdavit-worker-6.internal.cloudapp.net:8042/node/containerlogs/container_e07_1609265553881_0001_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fcbfd834cd0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import hsfs\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_2_code(x):\n",
    "    if (x == \"CASH_IN\"):\n",
    "        node_type = 0\n",
    "    elif (x == \"CASH_OUT\"):\n",
    "        node_type = 1\n",
    "    elif (x == \"DEBIT\"):\n",
    "        node_type = 2\n",
    "    elif (x == \"PAYMENT\"):\n",
    "        node_type = 3\n",
    "    elif (x == \"TRANSFER\"):\n",
    "        node_type = 4\n",
    "    elif (x == \"DEPOSIT\"):\n",
    "        node_type = 4        \n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    return dt_obj.timestamp()\n",
    "\n",
    "action_2_code_udf = F.udf(action_2_code)\n",
    "timestamp_2_time_udf = F.udf(timestamp_2_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to Hopsworks feature store (hsfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load accounts datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/accounts.csv\".format(hdfs.project_name()))\n",
    "\n",
    "accounts_df = accounts_df.drop('first_name')\\\n",
    "                        .drop('last_name')\\\n",
    "                        .drop('street_addr')\\\n",
    "                        .drop('city')\\\n",
    "                        .drop('state')\\\n",
    "                        .drop('zip')\\\n",
    "                        .drop('gender')\\\n",
    "                        .drop('birth_date')\\\n",
    "                        .drop('ssn')\\\n",
    "                        .drop('lon')\\\n",
    "                        .drop('lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.withColumn('prior_sar',F.when(F.col('prior_sar_count') == 'true', 1).otherwise(0))\\\n",
    "                         .drop(\"prior_sar_count\",\"acct_rptng_crncy\",\"type\",\"acct_stat\",\"open_dt\",\"bank_id\",\"country\",\"close_dt\",\"dsply_nm\",\"branch_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create accounts feature group metadata and save it in to hsfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fcc09116690>"
     ]
    }
   ],
   "source": [
    "accounts_fg = fs.create_feature_group(name=\"account_features\",\n",
    "                                      version=1,\n",
    "                                      primary_key=[\"acct_id\"],\n",
    "                                      description=\"node features\",\n",
    "                                      time_travel_format=None,\n",
    "                                      statistics_config=False)\n",
    "accounts_fg.save(accounts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/transactions.csv\".format(hdfs.project_name()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------+------+--------+-------+-------+--------+\n",
      "|source|target|tran_timestamp|is_sar|alert_id|tran_id|tx_type|base_amt|\n",
      "+------+------+--------------+------+--------+-------+-------+--------+\n",
      "|   218|    78|   1.4832288E9|     0|      -1|      1|      4|  458.69|\n",
      "|   213|    95|   1.4832288E9|     0|      -1|      2|      4|  537.69|\n",
      "|   191|    74|   1.4832288E9|     0|      -1|      3|      4|  139.61|\n",
      "|   166|   197|   1.4832288E9|     0|      -1|      4|      4|  717.61|\n",
      "|    16|    46|   1.4832288E9|     0|      -1|      5|      4|  275.56|\n",
      "|   100|   296|   1.4832288E9|     0|      -1|      6|      4|  870.63|\n",
      "|   202|    76|   1.4832288E9|     1|       0|      7|      4|  157.52|\n",
      "|    69|   229|   1.4832288E9|     0|      -1|      8|      4|  498.12|\n",
      "|    97|   121|   1.4832288E9|     0|      -1|      9|      4|  451.32|\n",
      "|     9|    62|   1.4832288E9|     0|      -1|     10|      4|  688.63|\n",
      "|   118|    77|   1.4832288E9|     0|      -1|     11|      4|  395.89|\n",
      "|    98|   162|   1.4832288E9|     0|      -1|     12|      4|  153.48|\n",
      "|    35|   215|   1.4832288E9|     0|      -1|     13|      4|  957.03|\n",
      "|   266|   276|   1.4832288E9|     0|      -1|     14|      4|  845.54|\n",
      "|   249|   281|   1.4832288E9|     0|      -1|     15|      4|  198.14|\n",
      "|   295|    78|   1.4832288E9|     0|      -1|     16|      4|  589.35|\n",
      "|    22|   168|   1.4832288E9|     0|      -1|     17|      4|  570.68|\n",
      "|   273|   261|   1.4832288E9|     0|      -1|     18|      4|  488.36|\n",
      "|   260|   127|   1.4832288E9|     0|      -1|     19|      4|   640.2|\n",
      "|   244|   281|   1.4832288E9|     0|      -1|     20|      4|  286.85|\n",
      "+------+------+--------------+------+--------+-------+-------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('is_sar',F.when(F.col('is_sar') == 'true', 1).otherwise(0))\\\n",
    "                                 .withColumn('tx_type', action_2_code_udf(F.col('tx_type')))\\\n",
    "                                 .withColumn('tran_timestamp', timestamp_2_time_udf(F.col('tran_timestamp')).cast(FloatType()))\\\n",
    "                                 .withColumnRenamed(\"orig_acct\",\"source\")\\\n",
    "                                 .withColumnRenamed(\"bene_acct\",\"target\")\\\n",
    "                                 .select(\"source\",\"target\",\"tran_timestamp\",\"is_sar\",\"alert_id\",\"tran_id\",\"tx_type\",\"base_amt\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transactions feature group metadata and save it in to hsfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fcc08cb6dd0>"
     ]
    }
   ],
   "source": [
    "transactions_fg = fs.create_feature_group(name=\"transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "                                       description=\"edge features\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "transactions_fg.save(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
