{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>22</td><td>application_1609605496842_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworksdavit-master.internal.cloudapp.net:8088/proxy/application_1609605496842_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworksdavit-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e04_1609605496842_0004_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f2c9e97f410>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "import hsfs\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_2_code(input_str):\n",
    "    x = input_str.split(\"-\")[0]\n",
    "    if (x == \"CASH_IN\"):\n",
    "        node_type = 0\n",
    "    elif (x == \"CASH_OUT\"):\n",
    "        node_type = 1\n",
    "    elif (x == \"DEBIT\"):\n",
    "        node_type = 2\n",
    "    elif (x == \"PAYMENT\"):\n",
    "        node_type = 3\n",
    "    elif (x == \"TRANSFER\"):\n",
    "        node_type = 4\n",
    "    elif (x == \"DEPOSIT\"):\n",
    "        node_type = 4        \n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    return dt_obj.strftime(\"%b-%d\") \n",
    "\n",
    "action_2_code_udf = F.udf(action_2_code)\n",
    "timestamp_2_time_udf = F.udf(timestamp_2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to Hopsworks feature store (hsfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/transactions.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------+-------------------+------+--------+--------+--------+\n",
      "|tran_id|         tx_type|base_amt|     tran_timestamp|is_sar|alert_id|     src|     dst|\n",
      "+-------+----------------+--------+-------------------+------+--------+--------+--------+\n",
      "|    496| TRANSFER-FanOut|  858.77|2020-01-01 00:00:00| false|      -1|3aa9646b|1e46e726|\n",
      "|   1342| TRANSFER-Mutual|  386.86|2020-01-01 00:00:00| false|      -1|49203bc3|a74d1101|\n",
      "|   1580| TRANSFER-FanOut|  616.43|2020-01-02 00:00:00| false|      -1|616d4505|99af2455|\n",
      "|   2866| TRANSFER-FanOut|  146.44|2020-01-02 00:00:00| false|      -1|39be1ea2|e7ec7bdb|\n",
      "|   3997| TRANSFER-Mutual|  439.09|2020-01-03 00:00:00| false|      -1|e2e0d938|afc399a9|\n",
      "|   5518| TRANSFER-FanOut|   361.0|2020-01-04 00:00:00| false|      -1|75c9a805|d7a317f6|\n",
      "|   7340| TRANSFER-Mutual|  768.98|2020-01-06 00:00:00| false|      -1|c14f4989|733a496b|\n",
      "|   9376| TRANSFER-FanOut|   943.4|2020-01-07 00:00:00| false|      -1|576eb672|aa49b0eb|\n",
      "|  10362|TRANSFER-Forward|   668.3|2020-01-08 00:00:00| false|      -1|847a9cf6|b070a6bb|\n",
      "|  10817| TRANSFER-FanOut|  139.84|2020-01-08 00:00:00| false|      -1|12a388ff|586377aa|\n",
      "|  11317| TRANSFER-Mutual|  499.47|2020-01-08 00:00:00| false|      -1|b36f9c84|1b467848|\n",
      "|  11748|TRANSFER-Forward|  357.96|2020-01-09 00:00:00| false|      -1|362e42e0|385afb8b|\n",
      "|  13285|TRANSFER-Forward|   630.9|2020-01-10 00:00:00| false|      -1|572014da|acd60eca|\n",
      "|  14832| TRANSFER-Mutual|  685.07|2020-01-11 00:00:00| false|      -1|5ff2d9a7|31976e38|\n",
      "|  15619| TRANSFER-FanOut|  964.81|2020-01-11 00:00:00| false|      -1|24bf603c|fcf3bbf3|\n",
      "|  16574|TRANSFER-Forward|  919.76|2020-01-12 00:00:00| false|      -1|9a118f8d|ca0967a6|\n",
      "|  18944| TRANSFER-FanOut|  302.26|2020-01-14 00:00:00| false|      -1|65b8a85f|bc0de3c7|\n",
      "|  19204|TRANSFER-Forward|  637.18|2020-01-14 00:00:00| false|      -1|d7a317f6|31db9495|\n",
      "|  19530| TRANSFER-Mutual|  630.43|2020-01-14 00:00:00| false|      -1|95aac0c4|3a63a8fc|\n",
      "|  20382| TRANSFER-Mutual|  374.73|2020-01-15 00:00:00| false|      -1|5411dc34|f388cc0f|\n",
      "+-------+----------------+--------+-------------------+------+--------+--------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------+-------+-------+--------+\n",
      "|     src|     dst|tran_timestamp|tran_id|tx_type|base_amt|\n",
      "+--------+--------+--------------+-------+-------+--------+\n",
      "|3aa9646b|1e46e726|        Jan-01|    496|      4|  858.77|\n",
      "|49203bc3|a74d1101|        Jan-01|   1342|      4|  386.86|\n",
      "|616d4505|99af2455|        Jan-02|   1580|      4|  616.43|\n",
      "|39be1ea2|e7ec7bdb|        Jan-02|   2866|      4|  146.44|\n",
      "|e2e0d938|afc399a9|        Jan-03|   3997|      4|  439.09|\n",
      "|75c9a805|d7a317f6|        Jan-04|   5518|      4|   361.0|\n",
      "|c14f4989|733a496b|        Jan-06|   7340|      4|  768.98|\n",
      "|576eb672|aa49b0eb|        Jan-07|   9376|      4|   943.4|\n",
      "|847a9cf6|b070a6bb|        Jan-08|  10362|      4|   668.3|\n",
      "|12a388ff|586377aa|        Jan-08|  10817|      4|  139.84|\n",
      "|b36f9c84|1b467848|        Jan-08|  11317|      4|  499.47|\n",
      "|362e42e0|385afb8b|        Jan-09|  11748|      4|  357.96|\n",
      "|572014da|acd60eca|        Jan-10|  13285|      4|   630.9|\n",
      "|5ff2d9a7|31976e38|        Jan-11|  14832|      4|  685.07|\n",
      "|24bf603c|fcf3bbf3|        Jan-11|  15619|      4|  964.81|\n",
      "|9a118f8d|ca0967a6|        Jan-12|  16574|      4|  919.76|\n",
      "|65b8a85f|bc0de3c7|        Jan-14|  18944|      4|  302.26|\n",
      "|d7a317f6|31db9495|        Jan-14|  19204|      4|  637.18|\n",
      "|95aac0c4|3a63a8fc|        Jan-14|  19530|      4|  630.43|\n",
      "|5411dc34|f388cc0f|        Jan-15|  20382|      4|  374.73|\n",
      "+--------+--------+--------------+-------+-------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('tx_type', action_2_code_udf(F.col('tx_type')))\\\n",
    "                                 .withColumn('tran_timestamp', timestamp_2_time_udf(F.col('tran_timestamp')).cast(StringType()))\\\n",
    "                                 .withColumnRenamed(\"orig_acct\",\"source\")\\\n",
    "                                 .withColumnRenamed(\"bene_acct\",\"target\")\\\n",
    "                                 .select(\"src\",\"dst\",\"tran_timestamp\",\"tran_id\",\"tx_type\",\"base_amt\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transactions feature group metadata and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_fg = fs.create_feature_group(name=\"transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "#                                       partition_key=[\"tran_timestamp\"],   \n",
    "                                       description=\"transactions features\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "transactions_fg.save(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load alert transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/alert_transactions.csv\".format(hdfs.project_name()))\n",
    "alert_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = alert_transactions.select(\"alert_id\",\"alert_type\",\"is_sar\",\"tran_id\").orderBy(\"tran_id\")\n",
    "alert_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create alert transactions feature group metadata and save it in to hsfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions_fg = fs.create_feature_group(name=\"alert_transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "#                                       partition_key=[\"alert_type\"],         \n",
    "                                       description=\"alert transactions\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "alert_transactions_fg.save(alert_transactions_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
