{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>12</td><td>application_1607211657348_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607211657348_0014/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1607211657348_0014_01_000001/amlsim2__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fc706cffed0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pydoop.hdfs as pydoop\n",
    "from model.gan_enc_ano import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_td = fs.get_training_dataset(\"ben_td\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = ben_td.tf_data(target_name='target', is_training=True)\n",
    "train_input_processed = train_input.tf_record_dataset(process=True, batch_size=32, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.core.tfdata_engine.TFDataEngine object at 0x7fc694674ad0>"
     ]
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((32, 5, 365), (32,)), types: (tf.float32, tf.float32)>"
     ]
    }
   ],
   "source": [
    "train_input_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"distcriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "real_inputs (InputLayer)     [(None, 365)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 365)               133590    \n",
      "_________________________________________________________________\n",
      "d_out (Dense)                (None, 1)                 366       \n",
      "=================================================================\n",
      "Total params: 133,956\n",
      "Trainable params: 133,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________"
     ]
    }
   ],
   "source": [
    "d_model = get_discriminator_model(model_name=\"distcriminator\", \n",
    "                                  input_name=\"real_inputs\", \n",
    "                                  input_dim=365, \n",
    "                                  output_name=\"d_out\", \n",
    "                                  output_dim=1, \n",
    "                                  n_units=365,\n",
    "                                  n_layers=1, \n",
    "                                  middle_layer_activation_fn=None, \n",
    "                                  final_activation_fn=\"tanh\", \n",
    "                                  double_neurons=False,\n",
    "                                  bottleneck_neurons=False,\n",
    "                                  batch_norm=False,\n",
    "                                  batch_dropout=False,\n",
    "                                  dropout_rate=0.0)\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fake_inputs (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "g_out (Dense)                (None, 365)               4015      \n",
      "=================================================================\n",
      "Total params: 4,105\n",
      "Trainable params: 4,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________"
     ]
    }
   ],
   "source": [
    "g_model = get_generator_model(model_name=\"generator\",\n",
    "                    input_name=\"fake_inputs\",\n",
    "                    noise_dim=8,\n",
    "                    output_name=\"g_out\",\n",
    "                    output_dim=365,\n",
    "                    n_units=10,\n",
    "                    n_layers=1,\n",
    "                    middle_layer_activation_fn=None,\n",
    "                    final_activation_fn=\"sigmoid\",\n",
    "                    double_neurons=False,\n",
    "                    bottleneck_neurons=False,\n",
    "                    batch_norm=False,\n",
    "                    batch_dropout=False,\n",
    "                    dropout_rate=0)\n",
    "\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  [(None, 365)]             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               46848     \n",
      "_________________________________________________________________\n",
      "e_out (Dense)                (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 47,880\n",
      "Trainable params: 47,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________"
     ]
    }
   ],
   "source": [
    "e_model = get_encoder_model(model_name=\"encoder\", \n",
    "                            input_name=\"encoder_inputs\", \n",
    "                            input_dim=365, \n",
    "                            output_name=\"e_out\", \n",
    "                            output_dim=8, \n",
    "                            n_units=128,\n",
    "                            n_layers=1, \n",
    "                            middle_layer_activation_fn=None, \n",
    "                            final_activation_fn=\"relu\", \n",
    "                            double_neurons=False,\n",
    "                            bottleneck_neurons=True, \n",
    "                            batch_norm=False, \n",
    "                            batch_dropout=False, \n",
    "                            dropout_rate=0.0)\n",
    "\n",
    "e_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "in user code:\n",
      "\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n",
      "        return fn(*args, **kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    /srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim2__meb10000/appcache/application_1607211657348_0014/container_e01_1607211657348_0014_01_000001/model.zip/model/gan_enc_ano.py:99 train_step\n",
      "        gp = self.gradient_penalty(real_data, fake_data)\n",
      "    /srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim2__meb10000/appcache/application_1607211657348_0014/container_e01_1607211657348_0014_01_000001/model.zip/model/gan_enc_ano.py:40 gradient_penalty\n",
      "        diff = fake_data - real_data\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1140 binary_op_wrapper\n",
      "        raise e\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1124 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:526 subtract\n",
      "        return gen_math_ops.sub(x, y, name)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py:10466 sub\n",
      "        \"Sub\", x=x, y=y, name=name)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n",
      "        attrs=attr_protos, op_def=op_def)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        compute_device)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n",
      "        op_def=op_def)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1975 __init__\n",
      "        control_input_ops, op_def)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: Dimensions must be equal, but are 32 and 5 for '{{node sub_1}} = Sub[T=DT_FLOAT](generator/g_out/Sigmoid, IteratorGetNext)' with input shapes: [32,365], [32,5,365].\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\n",
      "    *args, **kwds))\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n",
      "        return fn(*args, **kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    /srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim2__meb10000/appcache/application_1607211657348_0014/container_e01_1607211657348_0014_01_000001/model.zip/model/gan_enc_ano.py:99 train_step\n",
      "        gp = self.gradient_penalty(real_data, fake_data)\n",
      "    /srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim2__meb10000/appcache/application_1607211657348_0014/container_e01_1607211657348_0014_01_000001/model.zip/model/gan_enc_ano.py:40 gradient_penalty\n",
      "        diff = fake_data - real_data\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1140 binary_op_wrapper\n",
      "        raise e\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1124 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:526 subtract\n",
      "        return gen_math_ops.sub(x, y, name)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py:10466 sub\n",
      "        \"Sub\", x=x, y=y, name=name)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n",
      "        attrs=attr_protos, op_def=op_def)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal\n",
      "        compute_device)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n",
      "        op_def=op_def)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1975 __init__\n",
      "        control_input_ops, op_def)\n",
      "    /srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: Dimensions must be equal, but are 32 and 5 for '{{node sub_1}} = Sub[T=DT_FLOAT](generator/g_out/Sigmoid, IteratorGetNext)' with input shapes: [32,365], [32,5,365].\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs for trainining.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "#cbk = GANMonitor(num_sample=3, latent_dim=noise_dim)\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    encoder=e_model,\n",
    "    latent_dim=8,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile()\n",
    "\n",
    "# Start training the model.\n",
    "wgan.fit(train_input_processed) #, callbacks=[cbk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_main(\n",
    "\n",
    "             gp_w = 1,   # gp_weight\n",
    "             d_n = 2,    # d_n_neurons\n",
    "             d_n_l = 1,  # d_n_layers\n",
    "             d_a_f = 1,  # d_activation_fn\n",
    "             d_a_d_b = 1,  # d_double_neurons or d_bottlneck_neurons\n",
    "             d_b_n = 0,  # d_batch_norm\n",
    "             d_dr = 0,   # d_batch_dropout\n",
    "             d_b_r = 1,  # d_dropout_rate\n",
    "             d_lr = 1,   # d_learning_rate\n",
    "             d_k_b_r = 1, # d_kernel_bias_reg\n",
    "             d_l1_r = 1, # int_to_l1_rate\n",
    "             d_l2_r = 1, # int_to_l2_rate\n",
    "\n",
    "             g_n = 5,    # g_n_neurons\n",
    "             g_n_l = 1,  # g_n_layers\n",
    "             g_a_f = 1,  # g_activation_fn\n",
    "             g_a_d_b = 1,  # g_double_neurons or g_bottlneck_neurons\n",
    "             g_b_n = 0,  # g_batch_norm\n",
    "             g_dr = 0,   # g_batch_dropout\n",
    "             g_b_r =  1,  # g_dropout_rate\n",
    "             g_k_b_r = 1, # g_kernel_bias_reg\n",
    "             g_lr =  1,    # generator_lr\n",
    "             g_l1_r = 1, #g_l1_rate\n",
    "             g_l2_r = 1, #g_l2_rate\n",
    "\n",
    "             en_hp = 1  # encoder_n_layers        \n",
    "\n",
    "\n",
    "):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "    disable_eager_execution()\n",
    "    \n",
    "    #################\n",
    "    # Name cannot be empty, cannot exceed 256 characters and must match the regular expression: ^[a-zA-Z0-9]+$\n",
    "    model_name = 'ganomaly' \n",
    "    ################# \n",
    "    \n",
    "    int_to_neurons = {\n",
    "        1: 4096,\n",
    "        2: 2048,\n",
    "        3: 1024,\n",
    "        4: 512, \n",
    "        5: 256, \n",
    "        6: 128,\n",
    "        7: 64,\n",
    "        8: 32\n",
    "    }\n",
    "\n",
    "    int_to_learning_rate = {\n",
    "        1: 0.00001,\n",
    "        2: 0.001,\n",
    "        3: 0.0015,\n",
    "        4: 0.002,\n",
    "        5: 0.0025,\n",
    "        6: 0.003,\n",
    "        7: 0.0035,\n",
    "        8: 0.004,\n",
    "        9: 0.0045,\n",
    "        10: 0.005,\n",
    "        11: 0.0055,\n",
    "        12: 0.006,\n",
    "        13: 0.0065,\n",
    "        14: 0.007,\n",
    "        15: 0.0075,\n",
    "        16: 0.008,\n",
    "        17: 0.0085,\n",
    "        18: 0.009,\n",
    "        19: 0.0095,\n",
    "        20: 0.01, \n",
    "        21: 0.02, \n",
    "        22: 0.03, \n",
    "        23: 0.04 \n",
    "    }  \n",
    "    \n",
    "    int_to_dropout_rate= {\n",
    "        1: 0.1,\n",
    "        2: 0.15,\n",
    "        3: 0.2,\n",
    "        4: 0.25,\n",
    "        5: 0.3,\n",
    "        6: 0.35,\n",
    "        7: 0.4,\n",
    "        8: 0.45,\n",
    "        9: 0.5,\n",
    "        10: 0.55,\n",
    "        11: 0.6 \n",
    "    }\n",
    "    \n",
    "    int_to_activation_fn= {\n",
    "        1: 'relu',\n",
    "        2: 'leaky_relu',\n",
    "        3: 'tanh',\n",
    "        4: 'selu',\n",
    "        5: 'linear'\n",
    "    }\n",
    "    \n",
    "    int_to_l1_rate = {\n",
    "        1: 0.0001,\n",
    "        2: 0.0002,\n",
    "        3: 0.0003,\n",
    "        4: 0.0004,\n",
    "        5: 0.0005,\n",
    "        6: 0.0006,\n",
    "        7: 0.0007,\n",
    "        8: 0.0008,\n",
    "        9: 0.0009,\n",
    "        10: 0.001,\n",
    "        11: 0.002\n",
    "    }\n",
    "    \n",
    "    int_to_l2_rate = {\n",
    "        1: 0.4,\n",
    "        2: 0.45,\n",
    "        3: 0.5,\n",
    "        4: 0.55,\n",
    "        5: 0.6, \n",
    "        6: 0.65, \n",
    "        7: 0.7, \n",
    "        8: 0.75, \n",
    "        9: 0.8, \n",
    "        10: 0.9, \n",
    "        11: 0.95\n",
    "    }\n",
    "\n",
    "    int_to_gp_weight = {\n",
    "        1: 1.0,\n",
    "        2: 5.0,\n",
    "        3: 10.0\n",
    "    }\n",
    "    \n",
    "    d_arch_double_bottlneck = d_a_d_b\n",
    "    g_arch_double_bottlneck = g_a_d_b\n",
    "    \n",
    "    \n",
    "    # ML Hparams.\n",
    "    n_epochs=1\n",
    "    data_size=300\n",
    "    batch_size=16\n",
    "    #noise_dims=64 #int(int(int_to_neurons[g_n]/2)/2)\n",
    "    g_output_dim=365\n",
    "    d_output_dim=1\n",
    "    feature_dim=1\n",
    "    time_steps=365\n",
    "    timeseries=False\n",
    "\n",
    "    gp_weight=int_to_gp_weight[gp_w]\n",
    "\n",
    "    d_n_neurons=int_to_neurons[d_n]\n",
    "    d_n_layers=d_n_l\n",
    "    d_activation_fn=int_to_activation_fn[d_a_f]\n",
    "    if d_arch_double_bottlneck == 1: \n",
    "        d_double_neurons = True\n",
    "        d_bottlneck_neurons= False\n",
    "    elif d_arch_double_bottlneck == 2: \n",
    "        d_double_neurons = False\n",
    "        d_bottlneck_neurons= True\n",
    "    elif d_arch_double_bottlneck == 3: \n",
    "        d_double_neurons = False\n",
    "        d_bottlneck_neurons= False\n",
    "    \n",
    "    if d_b_n == 0:     \n",
    "        d_batch_norm=False\n",
    "    else:\n",
    "        d_batch_norm=True\n",
    "    \n",
    "    if d_dr == 0:\n",
    "        d_batch_dropout=False\n",
    "    else:     \n",
    "        d_batch_dropout=True\n",
    "        \n",
    "    d_dropout_rate=int_to_dropout_rate[d_b_r]\n",
    "    d_kernel_bias_reg=d_k_b_r\n",
    "    discriminator_lr=int_to_learning_rate[d_lr]\n",
    "    d_l1_rate=int_to_l1_rate[d_l1_r]\n",
    "    d_l2_rate=int_to_l2_rate[d_l2_r]\n",
    "\n",
    "    g_n_neurons=int_to_neurons[g_n]\n",
    "    g_n_layers=g_n_l\n",
    "    g_activation_fn=int_to_activation_fn[g_a_f]\n",
    "    if g_arch_double_bottlneck == 1: \n",
    "        g_double_neurons = True\n",
    "        g_bottlneck_neurons= False\n",
    "    elif g_arch_double_bottlneck == 2: \n",
    "        g_double_neurons = False\n",
    "        g_bottlneck_neurons= True\n",
    "    elif g_arch_double_bottlneck == 3: \n",
    "        g_double_neurons = False\n",
    "        g_bottlneck_neurons= False\n",
    "            \n",
    "    if g_b_n == 0:\n",
    "        g_batch_norm=False\n",
    "    else:\n",
    "        g_batch_norm=True\n",
    "    \n",
    "    if g_dr == 0:\n",
    "        g_batch_dropout=False\n",
    "    else: \n",
    "        g_batch_dropout=True\n",
    "        \n",
    "    g_dropout_rate=int_to_dropout_rate[g_b_r]\n",
    "    g_kernel_bias_reg=g_k_b_r\n",
    "    generator_lr=int_to_learning_rate[g_lr]\n",
    "    g_l1_rate=int_to_l1_rate[g_l1_r]\n",
    "    g_l2_rate=int_to_l2_rate[g_l2_r]\n",
    "\n",
    "    #################\n",
    "    # here I privide 8 options for each to not accidentally fail durring hp tuning\n",
    "    encoder_noise_hp_dict = \\\n",
    "    {1: {\n",
    "        1:{'noise_dims':8,'encoder_start_num_neurons':128, 'encoder_n_layers':5},\n",
    "        2:{'noise_dims':16,'encoder_start_num_neurons':128, 'encoder_n_layers':4},\n",
    "        3:{'noise_dims':32,'encoder_start_num_neurons':128, 'encoder_n_layers':3},\n",
    "        4:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        5:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        6:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        7:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':64,'encoder_start_num_neurons':128, 'encoder_n_layers':2}\n",
    "    },\n",
    "\n",
    "    2: {\n",
    "        1:{'noise_dims':8,'encoder_start_num_neurons':256, 'encoder_n_layers':6},\n",
    "        2:{'noise_dims':16,'encoder_start_num_neurons':256, 'encoder_n_layers':5},\n",
    "        3:{'noise_dims':32,'encoder_start_num_neurons':256, 'encoder_n_layers':4},\n",
    "        4:{'noise_dims':64,'encoder_start_num_neurons':256, 'encoder_n_layers':3},\n",
    "        5:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2},\n",
    "        6:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2},\n",
    "        7:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':128,'encoder_start_num_neurons':256, 'encoder_n_layers':2}        \n",
    "    }, \n",
    "\n",
    "    3: {\n",
    "        1:{'noise_dims':8, 'encoder_start_num_neurons':512, 'encoder_n_layers':7},\n",
    "        2:{'noise_dims':16, 'encoder_start_num_neurons':512, 'encoder_n_layers':6},\n",
    "        3:{'noise_dims':32, 'encoder_start_num_neurons':512, 'encoder_n_layers':5},\n",
    "        4:{'noise_dims':64, 'encoder_start_num_neurons':512, 'encoder_n_layers':4},\n",
    "        5:{'noise_dims':128, 'encoder_start_num_neurons':512, 'encoder_n_layers':3},\n",
    "        6:{'noise_dims':256, 'encoder_start_num_neurons':512, 'encoder_n_layers':2},\n",
    "        7:{'noise_dims':256, 'encoder_start_num_neurons':512, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':256, 'encoder_start_num_neurons':512, 'encoder_n_layers':2}        \n",
    "    }, \n",
    "\n",
    "    4: {\n",
    "        1:{'noise_dim':8,'encoder_start_num_neurons':1024, 'encoder_n_layers':8},\n",
    "        2:{'noise_dims':16,'encoder_start_num_neurons':1024, 'encoder_n_layers':7},\n",
    "        3:{'noise_dims':32,'encoder_start_num_neurons':1024, 'encoder_n_layers':6},\n",
    "        4:{'noise_dims':64,'encoder_start_num_neurons':1024, 'encoder_n_layers':5},\n",
    "        5:{'noise_dims':128,'encoder_start_num_neurons':1024, 'encoder_n_layers':4},\n",
    "        6:{'noise_dims':256,'encoder_start_num_neurons':1024, 'encoder_n_layers':3},        \n",
    "        7:{'noise_dims':512,'encoder_start_num_neurons':1024, 'encoder_n_layers':2},\n",
    "        8:{'noise_dims':512,'encoder_start_num_neurons':1024, 'encoder_n_layers':2}\n",
    "    },\n",
    "\n",
    "    5: {\n",
    "        1:{'noise_dims':8, 'encoder_start_num_neurons':2048, 'encoder_n_layers':9},\n",
    "        2:{'noise_dims':16, 'encoder_start_num_neurons':2048, 'encoder_n_layers':8},\n",
    "        3:{'noise_dims':32, 'encoder_start_num_neurons':2048, 'encoder_n_layers':7},\n",
    "        4:{'noise_dims':64, 'encoder_start_num_neurons':2048, 'encoder_n_layers':6},\n",
    "        5:{'noise_dims':128, 'encoder_start_num_neurons':2048, 'encoder_n_layers':5},\n",
    "        6:{'noise_dims':256, 'encoder_start_num_neurons':2048, 'encoder_n_layers':4},\n",
    "        7:{'noise_dims':512, 'encoder_start_num_neurons':2048, 'encoder_n_layers':3},\n",
    "        8:{'noise_dims':1024, 'encoder_start_num_neurons':2048, 'encoder_n_layers':2}\n",
    "    }}\n",
    "\n",
    "#     # all these variables are very realate so we will select  generator noise dimentions \n",
    "#     # based on combination between g_n_neurons and encoder_n_layers. encoder encoder_start_num_neurons\n",
    "#     # will be the same as g_n_neurons       \n",
    "\n",
    "    if g_n_neurons == 2048:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[5][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 1024:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[4][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 512:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[3][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 256:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[2][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    elif g_n_neurons == 128:\n",
    "        encoder_noise_hp = encoder_noise_hp_dict[1][en_hp]\n",
    "        encoder_start_num_neurons = encoder_noise_hp['encoder_start_num_neurons']\n",
    "        encoder_n_layers = encoder_noise_hp['encoder_n_layers'] \n",
    "        noise_dims = encoder_noise_hp['noise_dims']\n",
    "    #################\n",
    "    \n",
    "    joint_train=False\n",
    "\n",
    "    # ML Infra.\n",
    "    experiment_type='train'\n",
    "    #model_dir=logdir\n",
    "    num_gpus_per_worker=1 #hops.devices.get_num_gpus()\n",
    "    num_train_steps=50\n",
    "    num_eval_steps=1\n",
    "\n",
    "    num_summary_steps=1\n",
    "    log_step_count_steps=1\n",
    "    save_checkpoints_steps=1\n",
    "        \n",
    "    num_reader_parallel_calls=1\n",
    "    use_dummy_data=False\n",
    "    \n",
    "    #########################################################\n",
    "    data_dir = \"hdfs:///Projects/amlsim/gan_sim/\"\n",
    "    ben_dataset_dir = pydoop.path.abspath(data_dir + \"train.tfrecord\")\n",
    "    ben_input_files = tf.io.gfile.glob(ben_dataset_dir + \"/part-r-*\")\n",
    "    eval_dataset_dir = pydoop.path.abspath(data_dir + \"eval.tfrecord\") \n",
    "    eval_input_files = tf.io.gfile.glob(eval_dataset_dir + \"/part-r-*\")\n",
    "    \n",
    "    training_dataset = ben_input_files\n",
    "    eval_dataset = eval_input_files\n",
    "    label_name = \"target\"\n",
    "    #########################################################\n",
    "        \n",
    "    hparams = train_gan_enc_experiment_lib.HParams(\n",
    "        \n",
    "      model_name,  \n",
    "\n",
    "      n_epochs,\n",
    "      data_size,\n",
    "      batch_size,\n",
    "      noise_dims,\n",
    "      g_output_dim,\n",
    "      d_output_dim,\n",
    "      feature_dim,\n",
    "      time_steps,\n",
    "      timeseries,\n",
    "\n",
    "      gp_weight,\n",
    "\n",
    "      d_n_neurons,\n",
    "      d_n_layers,\n",
    "      d_activation_fn,\n",
    "      d_double_neurons,\n",
    "      d_bottlneck_neurons,\n",
    "      d_batch_norm,\n",
    "      d_batch_dropout,\n",
    "      d_dropout_rate,\n",
    "      d_kernel_bias_reg,\n",
    "      discriminator_lr,\n",
    "      d_l1_rate,\n",
    "      d_l2_rate,\n",
    "\n",
    "      g_n_neurons,\n",
    "      g_n_layers,\n",
    "      g_activation_fn,\n",
    "      g_double_neurons,\n",
    "      g_bottlneck_neurons,\n",
    "      g_batch_norm,\n",
    "      g_batch_dropout,\n",
    "      g_dropout_rate,\n",
    "      g_kernel_bias_reg,\n",
    "      generator_lr,\n",
    "      g_l1_rate,\n",
    "      g_l2_rate,\n",
    "\n",
    "      encoder_start_num_neurons,\n",
    "      encoder_n_layers,\n",
    "\n",
    "      joint_train,\n",
    "\n",
    "      experiment_type,\n",
    "\n",
    "      num_train_steps,\n",
    "      num_eval_steps,\n",
    "      num_summary_steps,\n",
    "      log_step_count_steps,\n",
    "      save_checkpoints_steps,\n",
    "\n",
    "      training_dataset, \n",
    "      eval_dataset,\n",
    "      label_name,  \n",
    "        \n",
    "      num_reader_parallel_calls,\n",
    "      use_dummy_data,\n",
    "        \n",
    "      0.5,\n",
    "      1   \n",
    "    )\n",
    "    \n",
    "    eval_result = train_gan_enc_experiment_lib.train(hparams)\n",
    "    return eval_result\n",
    "#     reporter.broadcast(metric=eval_result[\"loss\"])\n",
    "#     return eval_result[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hops import experiment\n",
    "#from hops.experiment import Direction\n",
    "#def hyperparam_search():\n",
    "#    search_dict = {\n",
    "#        \n",
    "#                'gp_w': [1,3],   # gp_weight\n",
    "#                'd_n': [2,4],    # d_n_neurons\n",
    "#                'd_n_l': [1,4],  # d_n_layers\n",
    "#                'd_a_f': [1,4],  # d_activation_fn\n",
    "#                'd_a_d_b': [1,3],  # d_double_neurons or d_bottlneck_neurons\n",
    "#                'd_b_n': [0,1],  # d_batch_norm\n",
    "#                'd_dr': [0,1],   # d_batch_dropout\n",
    "#                'd_b_r': [1,4],  # d_dropout_rate\n",
    "#                'd_lr': [1,4],   # d_learning_rate\n",
    "#                'd_k_b_r': [1,4], # d_kernel_bias_reg\n",
    "#                'd_l1_r': [1,4], # int_to_l1_rate\n",
    "#                'd_l2_r': [1,4], # int_to_l2_rate\n",
    "#\n",
    "#                'g_n': [5,6],    # g_n_neurons\n",
    "#                'g_n_l': [1,4],  # g_n_layers\n",
    "#                'g_a_f': [1,4],  # g_activation_fn\n",
    "#                'g_a_d_b': [1,3],  # g_double_neurons or g_bottlneck_neurons\n",
    "#                'g_b_n': [0,1],  # g_batch_norm\n",
    "#                'g_dr': [0,1],   # g_batch_dropout\n",
    "#                'g_b_r': [1,4],  # g_dropout_rate\n",
    "#                'g_k_b_r': [1,4], # g_kernel_bias_reg\n",
    "#                'g_lr': [1,4],    # generator_lr\n",
    "#                'g_l1_r': [1,4], #g_l1_rate\n",
    "#                'g_l2_r': [1,4], #g_l2_rate\n",
    "#\n",
    "#                'en_hp': [1,8]  # encoder_n_layers        \n",
    "#\n",
    "#    }\n",
    "#    \n",
    "#    log_dir, best_params = experiment.differential_evolution(\n",
    "#    gan_main, \n",
    "#    search_dict, \n",
    "#    name='gan_enc_search', \n",
    "#    description='GAN anomaly encoder search',\n",
    "#    local_logdir=True, \n",
    "#    population=8,\n",
    "#    generations = 10,\n",
    "#    direction=Direction.MIN, \n",
    "#    optimization_key='loss'    \n",
    "#    )\n",
    "#    return log_dir, best_params\n",
    "#\n",
    "#log_dir, best_params = hyperparam_search()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, ip-10-0-0-88.us-west-2.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 133, in _try_put\n",
      "    self._event_queue.put(item)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 287, in put\n",
      "    raise QueueClosedError()\n",
      "tensorflow.python.summary.writer.event_file_writer.QueueClosedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/experiment_impl/launcher.py\", line 143, in _wrapper_fun\n",
      "    retval = train_fn()\n",
      "  File \"<stdin>\", line 394, in gan_main\n",
      "  File \"/srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim__meb10179/appcache/application_1606981250434_0001/container_e03_1606981250434_0001_01_000003/tensorflow_gan.zip/tensorflow_gan/examples/anomaly/train_gan_enc_experiment_lib.py\", line 294, in train\n",
      "    tf.compat.v1.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 505, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 646, in run\n",
      "    return self.run_local()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 747, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1173, in _train_model\n",
      "    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1235, in _train_model_distributed\n",
      "    self._config._train_distribute, input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1349, in _actual_train_model_distributed\n",
      "    saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1507, in _train_with_estimator_spec\n",
      "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 737, in __init__\n",
      "    h.begin()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 563, in begin\n",
      "    self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer_cache.py\", line 63, in get\n",
      "    logdir, graph=ops.get_default_graph())\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 371, in __init__\n",
      "    super(FileWriter, self).__init__(event_writer, graph, graph_def)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 91, in __init__\n",
      "    maybe_graph_as_def))\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 249, in add_meta_graph\n",
      "    self._add_event(event, global_step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 393, in _add_event\n",
      "    super(FileWriter, self)._add_event(event, step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 279, in _add_event\n",
      "    self.event_writer.add_event(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 121, in add_event\n",
      "    self._try_put(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 135, in _try_put\n",
      "    self._internal_close()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 168, in _internal_close\n",
      "    self._ev_writer.Close()\n",
      "RuntimeError: hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1606981250434_0001_2/events.out.tfevents.1606988569.ip-10-0-0-88; Unknown error 255\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 133, in _try_put\n",
      "    self._event_queue.put(item)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 287, in put\n",
      "    raise QueueClosedError()\n",
      "tensorflow.python.summary.writer.event_file_writer.QueueClosedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/experiment_impl/launcher.py\", line 143, in _wrapper_fun\n",
      "    retval = train_fn()\n",
      "  File \"<stdin>\", line 394, in gan_main\n",
      "  File \"/srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim__meb10179/appcache/application_1606981250434_0001/container_e03_1606981250434_0001_01_000003/tensorflow_gan.zip/tensorflow_gan/examples/anomaly/train_gan_enc_experiment_lib.py\", line 294, in train\n",
      "    tf.compat.v1.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 505, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 646, in run\n",
      "    return self.run_local()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 747, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1173, in _train_model\n",
      "    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1235, in _train_model_distributed\n",
      "    self._config._train_distribute, input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1349, in _actual_train_model_distributed\n",
      "    saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1507, in _train_with_estimator_spec\n",
      "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 737, in __init__\n",
      "    h.begin()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 563, in begin\n",
      "    self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer_cache.py\", line 63, in get\n",
      "    logdir, graph=ops.get_default_graph())\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 371, in __init__\n",
      "    super(FileWriter, self).__init__(event_writer, graph, graph_def)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 91, in __init__\n",
      "    maybe_graph_as_def))\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 249, in add_meta_graph\n",
      "    self._add_event(event, global_step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 393, in _add_event\n",
      "    super(FileWriter, self)._add_event(event, step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 279, in _add_event\n",
      "    self.event_writer.add_event(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 121, in add_event\n",
      "    self._try_put(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 135, in _try_put\n",
      "    self._internal_close()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 168, in _internal_close\n",
      "    self._ev_writer.Close()\n",
      "RuntimeError: hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1606981250434_0001_2/events.out.tfevents.1606988569.ip-10-0-0-88; Unknown error 255\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/experiment.py\", line 110, in launch\n",
      "    logdir, return_dict = launcher._run(sc, train_fn, run_id, args_dict, local_logdir)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/experiment_impl/launcher.py\", line 47, in _run\n",
      "    nodeRDD.foreachPartition(_prepare_func(app_id, run_id, train_fn, args_dict, local_logdir))\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 806, in foreachPartition\n",
      "    self.mapPartitions(func).count()  # Force evaluation\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1055, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1046, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 917, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 816, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, ip-10-0-0-88.us-west-2.compute.internal, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 133, in _try_put\n",
      "    self._event_queue.put(item)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 287, in put\n",
      "    raise QueueClosedError()\n",
      "tensorflow.python.summary.writer.event_file_writer.QueueClosedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/experiment_impl/launcher.py\", line 143, in _wrapper_fun\n",
      "    retval = train_fn()\n",
      "  File \"<stdin>\", line 394, in gan_main\n",
      "  File \"/srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim__meb10179/appcache/application_1606981250434_0001/container_e03_1606981250434_0001_01_000003/tensorflow_gan.zip/tensorflow_gan/examples/anomaly/train_gan_enc_experiment_lib.py\", line 294, in train\n",
      "    tf.compat.v1.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 505, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 646, in run\n",
      "    return self.run_local()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 747, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1173, in _train_model\n",
      "    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1235, in _train_model_distributed\n",
      "    self._config._train_distribute, input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1349, in _actual_train_model_distributed\n",
      "    saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1507, in _train_with_estimator_spec\n",
      "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 737, in __init__\n",
      "    h.begin()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 563, in begin\n",
      "    self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer_cache.py\", line 63, in get\n",
      "    logdir, graph=ops.get_default_graph())\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 371, in __init__\n",
      "    super(FileWriter, self).__init__(event_writer, graph, graph_def)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 91, in __init__\n",
      "    maybe_graph_as_def))\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 249, in add_meta_graph\n",
      "    self._add_event(event, global_step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 393, in _add_event\n",
      "    super(FileWriter, self)._add_event(event, step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 279, in _add_event\n",
      "    self.event_writer.add_event(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 121, in add_event\n",
      "    self._try_put(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 135, in _try_put\n",
      "    self._internal_close()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 168, in _internal_close\n",
      "    self._ev_writer.Close()\n",
      "RuntimeError: hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1606981250434_0001_2/events.out.tfevents.1606988569.ip-10-0-0-88; Unknown error 255\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 133, in _try_put\n",
      "    self._event_queue.put(item)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 287, in put\n",
      "    raise QueueClosedError()\n",
      "tensorflow.python.summary.writer.event_file_writer.QueueClosedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/experiment_impl/launcher.py\", line 143, in _wrapper_fun\n",
      "    retval = train_fn()\n",
      "  File \"<stdin>\", line 394, in gan_main\n",
      "  File \"/srv/hops/hopsdata/tmp/nm-local-dir/usercache/amlsim__meb10179/appcache/application_1606981250434_0001/container_e03_1606981250434_0001_01_000003/tensorflow_gan.zip/tensorflow_gan/examples/anomaly/train_gan_enc_experiment_lib.py\", line 294, in train\n",
      "    tf.compat.v1.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 505, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 646, in run\n",
      "    return self.run_local()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 747, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1173, in _train_model\n",
      "    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1235, in _train_model_distributed\n",
      "    self._config._train_distribute, input_fn, hooks, saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1349, in _actual_train_model_distributed\n",
      "    saving_listeners)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1507, in _train_with_estimator_spec\n",
      "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 737, in __init__\n",
      "    h.begin()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 563, in begin\n",
      "    self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer_cache.py\", line 63, in get\n",
      "    logdir, graph=ops.get_default_graph())\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 371, in __init__\n",
      "    super(FileWriter, self).__init__(event_writer, graph, graph_def)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 91, in __init__\n",
      "    maybe_graph_as_def))\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 249, in add_meta_graph\n",
      "    self._add_event(event, global_step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 393, in _add_event\n",
      "    super(FileWriter, self)._add_event(event, step)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 279, in _add_event\n",
      "    self.event_writer.add_event(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 121, in add_event\n",
      "    self._try_put(event)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 135, in _try_put\n",
      "    self._internal_close()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 168, in _internal_close\n",
      "    self._ev_writer.Close()\n",
      "RuntimeError: hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1606981250434_0001_2/events.out.tfevents.1606988569.ip-10-0-0-88; Unknown error 255\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hops import experiment\n",
    "# experiment.collective_all_reduce(main)\n",
    "experiment.launch(gan_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Can't reach Maggy server. No progress information and logs available. Job continues running anyway.\n"
     ]
    }
   ],
   "source": [
    "# result = experiment.lagom(map_fun=main, \n",
    "#                            searchspace=sp, \n",
    "#                            optimizer='randomsearch', \n",
    "#                            direction='min',\n",
    "#                            num_trials=15, \n",
    "#                            name='gan_enc_search', \n",
    "#                            hb_interval=5, \n",
    "#                            es_interval=5,\n",
    "#                            es_min=5\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"ganomaly\"\n",
    "EVALUATION_METRIC=\"encoder_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "No model with name ganomaly and metric encoder_loss could be found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hops/model.py\", line 63, in get_best_model\n",
      "    raise ModelNotFound(\"No model with name {} and metric {} could be found.\".format(name, metric))\n",
      "hops.model.ModelNotFound: No model with name ganomaly and metric encoder_loss could be found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'best_model' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'best_model' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'best_model' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'best_model' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create serving\n",
    "model_path=\"/Models/\" + best_model['name']\n",
    "response = serving.create_or_update(model_path, MODEL_NAME, serving_type=\"TENSORFLOW\", \n",
    "                                 model_version=best_model['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available servings in the project\n",
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Stopped'"
     ]
    }
   ],
   "source": [
    "# Get serving status\n",
    "serving.get_status(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: ganomaly...\n",
      "Serving with name: ganomaly successfully started"
     ]
    }
   ],
   "source": [
    "if serving.get_status(MODEL_NAME) == 'Stopped':\n",
    "    serving.start(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(MODEL_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [1724.10559]}\n",
      "{'predictions': [347.750244]}\n",
      "{'predictions': [744.655]}\n",
      "{'predictions': [2042.27856]}\n",
      "{'predictions': [557.614258]}\n",
      "{'predictions': [315.85498]}\n",
      "{'predictions': [1655.20459]}\n",
      "{'predictions': [316.268921]}\n",
      "{'predictions': [1744.2124]}\n",
      "{'predictions': [1149.82349]}\n",
      "{'predictions': [1569.00781]}\n",
      "{'predictions': [468.74588]}\n",
      "{'predictions': [960.264221]}\n",
      "{'predictions': [1766.28601]}\n",
      "{'predictions': [985.822876]}\n",
      "{'predictions': [1204.39709]}\n",
      "{'predictions': [399.651917]}\n",
      "{'predictions': [440.584076]}\n",
      "{'predictions': [1225.974]}\n",
      "{'predictions': [208.740234]}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(20):\n",
    "    data = {\n",
    "                'serving_default': 'real_input',\n",
    "                \"instances\": [np.random.rand(365).astype(np.float32).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(MODEL_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.random.rand(365).astype(np.float32).tolist(),np.random.rand(365).astype(np.float32).tolist(),np.random.rand(365).astype(np.float32).tolist(), np.random.rand(365).astype(np.float32).tolist()]\n",
    "rdd = sc.parallelize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead"
     ]
    }
   ],
   "source": [
    "people = rdd.map(lambda x: { 'serving_default': 'real_input', \"instances\": [x] }).map(lambda x:  serving.make_inference_request(MODEL_NAME, x)).toDF()\n",
    "                 \n",
    "#rdd.map(lambda x: np.append(arr, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "| predictions|\n",
      "+------------+\n",
      "|[686.622559]|\n",
      "|[1406.82849]|\n",
      "|[681.662476]|\n",
      "|[520.253235]|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
