{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>3</td><td>application_1607211657348_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1607211657348_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1607211657348_0005_01_000001/amlsim2__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7f41f479\n"
     ]
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types.{FloatType, LongType}\n",
      "import org.apache.spark.sql.types.{ArrayType, StructField, StructType}\n",
      "import org.apache.spark.sql.functions.explode\n",
      "import org.apache.spark.sql.expressions.Window\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{FloatType,LongType}\n",
    "import org.apache.spark.sql.types.{ArrayType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions.explode\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.logicalclocks.hsfs._\n"
     ]
    }
   ],
   "source": [
    "import com.logicalclocks.hsfs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection: com.logicalclocks.hsfs.HopsworksConnection = com.logicalclocks.hsfs.HopsworksConnection@5ae6638a\n",
      "fs: com.logicalclocks.hsfs.FeatureStore = FeatureStore{id=129, name='amlsim2_featurestore', projectId=181, featureGroupApi=com.logicalclocks.hsfs.metadata.FeatureGroupApi@432a09d3}\n"
     ]
    }
   ],
   "source": [
    "val connection = HopsworksConnection.builder().build();\n",
    "val fs = connection.getFeatureStore();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [f_333: double, f_329: double ... 364 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df_all = fs.getFeatureGroup(\"simts_features\", 1).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Long = 11000\n"
     ]
    }
   ],
   "source": [
    "df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: Long = 1000\n"
     ]
    }
   ],
   "source": [
    "df_all.where($\"target\" === 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: Long = 10000\n"
     ]
    }
   ],
   "source": [
    "df_all.where($\"target\" === 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(f_333, f_329, f_328, f_327, f_320, f_312, f_307, f_302, f_301, f_299, f_296, f_293, f_286, f_285, f_281, f_280, f_277, f_266, f_261, f_256, f_255, f_254, f_253, f_252, f_251, f_249, f_240, f_235, f_232, f_230, f_228, f_226, f_222, f_215, f_211, f_209, f_207, f_193, f_186, f_185, f_177, f_175, f_173, f_172, f_170, f_169, f_163, f_162, f_158, f_155, f_153, f_152, f_151, f_146, f_142, f_141, f_140, f_138, f_132, f_130, f_127, f_124, f_117, f_114, f_111, f_110, f_106, f_104, f_103, f_102, f_100, f_97, f_94, f_84, f_80, f_79, f_78, f_77, f_65, f_62, f_60, f_59, f_55, f_50, f_49, f_46, f_41, f_40, f_34, f_32, f_24, f_23, f_17, f_14, f_11, f_10, f_8, f_4, f_3, f_334, f_341, f_348, f_350, f_358, f_365, f_323, f_321, f_317, f_311, f_297, f_295, f_292, f_287, f_276, f_270, f_269, f_267, f_265, f_258, f_242, f_233, f_229, f_217, f_216, f_208, f_205, f_204, f_203, f_202, f_201, f_200, f_196, f_195, f_191, f_184, f_183, f_182, f_181, f_179, f_174, f_167, f_164, f_159, f_156, f_154, f_148, f_147, f_145, f_143, f_139, f_136, f_133, f_123, f_120, f_113, f_112, f_101, f_99, f_98, f_95, f_93, f_87, f_76, f_75, f_74, f_72, f_71, f_69, f_68, f_64, f_63, f_61, f_52, f_51, f_48, f_47, f_44, f_42, f_38, f_37, f_36, f_35, f_30, f_29, f_22, f_20, f_19, f_16, f_13, f_12, f_6, f_1, f_331, f_335, f_340, f_343, f_344, f_345, f_353, f_359, f_361, target, f_357, f_356, f_355, f_352, f_349, f_347, f_346, f_339, f_338, f_337, f_336, f_330, f_325, f_324, f_322, f_319, f_318, f_315, f_309, f_308, f_306, f_304, f_303, f_300, f_298, f_284, f_279, f_278, f_272, f_268, f_263, f_260, f_259, f_257, f_246, f_245, f_244, f_241, f_238, f_237, f_227, f_225, f_224, f_221, f_220, f_218, f_214, f_213, f_212, f_210, f_206, f_199, f_188, f_176, f_168, f_165, f_160, f_157, f_149, f_137, f_135, f_126, f_122, f_119, f_118, f_116, f_115, f_91, f_89, f_83, f_66, f_54, f_45, f_43, f_28, f_26, f_25, f_9, f_2, f_364, f_363, f_362, f_360, f_354, f_351, f_342, f_332, f_326, f_316, f_314, f_313, f_310, f_305, f_294, f_291, f_290, f_289, f_288, f_283, f_282, f_275, f_274, f_273, f_271, f_264, f_262, f_250, f_248, f_247, f_243, f_239, f_236, f_234, f_231, f_223, f_219, f_198, f_197, f_194, f_192, f_190, f_189, f_187, f_180, f_178, f_171, f_166, f_161, f_150, f_144, f_134, f_131, f_129, f_128, f_125, f_121, f_109, f_108, f_107, f_105, f_96, f_92, f_90, f_88, f_86, f_85, f_82, f_81, f_73, f_70, f_67, f_58, f_57, f_56, f_53, f_39, f_33, f_31, f_27, f_21, f_18, f_15, f_7, f_5)"
     ]
    }
   ],
   "source": [
    "print(df_all.columns.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(f_333, f_329, f_328, f_327, f_320, f_312, f_307, f_302, f_301, f_299, f_296, f_293, f_286, f_285, f_281, f_280, f_277, f_266, f_261, f_256, f_255, f_254, f_253, f_252, f_251, f_249, f_240, f_235, f_232, f_230, f_228, f_226, f_222, f_215, f_211, f_209, f_207, f_193, f_186, f_185, f_177, f_175, f_173, f_172, f_170, f_169, f_163, f_162, f_158, f_155, f_153, f_152, f_151, f_146, f_142, f_141, f_140, f_138, f_132, f_130, f_127, f_124, f_117, f_114, f_111, f_110, f_106, f_104, f_103, f_102, f_100, f_97, f_94, f_84, f_80, f_79, f_78, f_77, f_65, f_62, f_60, f_59, f_55, f_50, f_49, f_46, f_41, f_40, f_34, f_32, f_24, f_23, f_17, f_14, f_11, f_10, f_8, f_4, f_3, f_334, f_341, f_348, f_350, f_358, f_365, f_323, f_321, f_317, f_311, f_297, f_295, f_292, f_287, f_276, f_270, f_269, f_267, f_265, f_258, f_242, f_233, f_229, f_217, f_216, f_208, f_205, f_204, f_203, f_202, f_201, f_200, f_196, f_195, f_191, f_184, f_183, f_182, f_181, f_179, f_174, f_167, f_164, f_159, f_156, f_154, f_148, f_147, f_145, f_143, f_139, f_136, f_133, f_123, f_120, f_113, f_112, f_101, f_99, f_98, f_95, f_93, f_87, f_76, f_75, f_74, f_72, f_71, f_69, f_68, f_64, f_63, f_61, f_52, f_51, f_48, f_47, f_44, f_42, f_38, f_37, f_36, f_35, f_30, f_29, f_22, f_20, f_19, f_16, f_13, f_12, f_6, f_1, f_331, f_335, f_340, f_343, f_344, f_345, f_353, f_359, f_361, target, f_357, f_356, f_355, f_352, f_349, f_347, f_346, f_339, f_338, f_337, f_336, f_330, f_325, f_324, f_322, f_319, f_318, f_315, f_309, f_308, f_306, f_304, f_303, f_300, f_298, f_284, f_279, f_278, f_272, f_268, f_263, f_260, f_259, f_257, f_246, f_245, f_244, f_241, f_238, f_237, f_227, f_225, f_224, f_221, f_220, f_218, f_214, f_213, f_212, f_210, f_206, f_199, f_188, f_176, f_168, f_165, f_160, f_157, f_149, f_137, f_135, f_126, f_122, f_119, f_118, f_116, f_115, f_91, f_89, f_83, f_66, f_54, f_45, f_43, f_28, f_26, f_25, f_9, f_2, f_364, f_363, f_362, f_360, f_354, f_351, f_342, f_332, f_326, f_316, f_314, f_313, f_310, f_305, f_294, f_291, f_290, f_289, f_288, f_283, f_282, f_275, f_274, f_273, f_271, f_264, f_262, f_250, f_248, f_247, f_243, f_239, f_236, f_234, f_231, f_223, f_219, f_198, f_197, f_194, f_192, f_190, f_189, f_187, f_180, f_178, f_171, f_166, f_161, f_150, f_144, f_134, f_131, f_129, f_128, f_125, f_121, f_109, f_108, f_107, f_105, f_96, f_92, f_90, f_88, f_86, f_85, f_82, f_81, f_73, f_70, f_67, f_58, f_57, f_56, f_53, f_39, f_33, f_31, f_27, f_21, f_18, f_15, f_7)"
     ]
    }
   ],
   "source": [
    "print(df_all.columns.slice(0,365).map(a => a.toString ).toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.{MinMaxScaler, StandardScaler, Normalizer, MaxAbsScaler}\n",
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4e292fce9fa6\n",
      "df_all_features: org.apache.spark.sql.DataFrame = [f_333: double, f_329: double ... 365 more fields]\n",
      "mms: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_d1c43ebf334d\n",
      "mms01: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_4c3208d654f5\n",
      "stds: org.apache.spark.ml.feature.StandardScaler = stdScal_ab92a94378f8\n",
      "maxabsscaler: org.apache.spark.ml.feature.MaxAbsScaler = maxAbsScal_e56c945c627c\n",
      "mms_tr: mms.type = minMaxScal_d1c43ebf334d\n",
      "mms01_tr: mms01.type = minMaxScal_4c3208d654f5\n",
      "mms_abs_tr: maxabsscaler.type = maxAbsScal_e56c945c627c\n",
      "stds_tr: stds.type = stdScal_ab92a94378f8\n",
      "trainingFeaturesPipeline: org.apache.spark.ml.Pipeline = pipeline_8d2e9b9342ce\n",
      "trainingFeaturesDF: org.apache.spark.sql.DataFrame = [f_333: double, f_329: double ... 369 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{MinMaxScaler,StandardScaler,Normalizer, MaxAbsScaler}\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(df_all.columns.slice(0,365).map(a => a.toString).toArray).setOutputCol(\"features\")\n",
    "\n",
    "val df_all_features = assembler.transform(df_all)\n",
    "\n",
    "// scaler instance with the min(-1) and max(1) \n",
    "val mms = new MinMaxScaler().\n",
    "    setInputCol(\"features\").\n",
    "    setOutputCol(\"mms\").\n",
    "    setMax(1).\n",
    "    setMin(-1)\n",
    "\n",
    "// scaler instance with the min(0) and max(1) \n",
    "val mms01 = new MinMaxScaler().\n",
    "    setInputCol(\"features\").\n",
    "    setOutputCol(\"mms\").\n",
    "    setMax(1).\n",
    "    setMin(0)\n",
    "\n",
    "val stds = new StandardScaler().\n",
    "    setInputCol(\"features\").\n",
    "    setOutputCol(\"stds\")\n",
    "\n",
    "val maxabsscaler = new MaxAbsScaler().\n",
    "    setInputCol(\"features\").\n",
    "    setOutputCol(\"maxabsscaler\")\n",
    "\n",
    "val mms_tr = mms.setInputCol(\"features\").setOutputCol(\"mms\")\n",
    "val mms01_tr = mms01.setInputCol(\"features\").setOutputCol(\"mms01\")\n",
    "val mms_abs_tr = maxabsscaler.setInputCol(\"features\").setOutputCol(\"maxabsscaler\")\n",
    "val stds_tr = stds.setInputCol(\"features\").setOutputCol(\"stds\")\n",
    "\n",
    "val trainingFeaturesPipeline = (new Pipeline()\n",
    "  .setStages(Array(mms_tr,mms01_tr,mms_abs_tr,stds_tr)))\n",
    "val trainingFeaturesDF = trainingFeaturesPipeline.fit(df_all_features).transform(df_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.linalg.DenseVector\n",
      "toArr: Any => Array[Double] = <function1>\n",
      "toArrUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(DoubleType,false),None)\n",
      "final_df_all: org.apache.spark.sql.DataFrame = [target: float, features: array<float> ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "val toArr: Any => Array[Double] = _.asInstanceOf[DenseVector].toArray\n",
    "val toArrUdf = udf(toArr)\n",
    "val final_df_all = trainingFeaturesDF.select($\"target\",$\"features\",$\"mms\",$\"mms01\",$\"maxabsscaler\",$\"stds\").\n",
    "             withColumn(\"target\",$\"target\".cast(FloatType)).\n",
    "             withColumn(\"features\",toArrUdf($\"features\").cast(ArrayType(FloatType))).\n",
    "             withColumn(\"mms\",toArrUdf($\"mms\").cast(ArrayType(FloatType))).\n",
    "             withColumn(\"mms01\",toArrUdf($\"mms01\").cast(ArrayType(FloatType))).\n",
    "             withColumn(\"maxabsscaler\",toArrUdf($\"maxabsscaler\").cast(ArrayType(FloatType))).\n",
    "             withColumn(\"stds\",toArrUdf($\"stds\").cast(ArrayType(FloatType)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, features: array<float> ... 4 more fields]\n",
      "ben: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, features: array<float> ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "val van = final_df_all.where($\"target\"===1)\n",
    "val ben = final_df_all.where($\"target\"===0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benTrainDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, features: array<float> ... 4 more fields]\n",
      "benEvalDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, features: array<float> ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "// Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "val Array(benTrainDF, benEvalDF) = final_df_all.randomSplit(Array(0.8, 0.02),seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res17: Long = 10728\n"
     ]
    }
   ],
   "source": [
    "benTrainDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18: Long = 272\n"
     ]
    }
   ],
   "source": [
    "benEvalDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, features: array<float> ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "val EvalDF = benEvalDF.union(van)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res19: Long = 1023\n"
     ]
    }
   ],
   "source": [
    "EvalDF.where($\"target\"===1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res21: Long = 249\n"
     ]
    }
   ],
   "source": [
    "EvalDF.where($\"target\"===0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@60349314\n"
     ]
    }
   ],
   "source": [
    "val ben_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"ben_td\")\n",
    "                .description(\"Ben simulated Time Series Training Dataset\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "ben_sim_ts_td_meta.save(benTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@243290ea\n"
     ]
    }
   ],
   "source": [
    "val eval_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"eval_td\")\n",
    "                .description(\"Simulated Time Series Training Dataset for evaluation\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "eval_sim_ts_td_meta.save(EvalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
